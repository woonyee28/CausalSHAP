{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from causal_inference import CausalInference\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "base_dir = '../../'\n",
    "result_dir = base_dir + 'result/R/'\n",
    "data_path = base_dir + 'dataset/' + 'Real_World_IBS.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "df = df.drop(columns=['HAD_Anxiety', 'Patient', 'Batch_metabolomics', 'BH', 'Sex', 'Age', 'BMI','Race','Education','HAD_Depression','STAI_Tanxiety', 'Diet_Category','Diet_Pattern'])\n",
    "label_encoder = LabelEncoder()\n",
    "df['Group'] = label_encoder.fit_transform(df['Group'])\n",
    "df_encoded = df\n",
    "\n",
    "X = df_encoded.drop(columns=['Group'])\n",
    "y = df_encoded['Group']\n",
    "\n",
    "X = X[[\"xylose\", \"xanthosine\", \"uracil\", \"ribulose/xylulose\", \"valylglutamine\",\n",
    "           \"tryptophylglycine\", \"succinate\", \"valine betaine\", \"ursodeoxycholate sulfate (1)\",\n",
    "           \"tricarballylate\", \"succinimide\", \"thymine\", \"syringic acid\", \"serotonin\", \"ribitol\"]]\n",
    "\n",
    "param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 7],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "estimator=rf, param_distributions=param_dist, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 54.6172 s\n",
      "File: c:\\Users\\snorl\\Desktop\\FYP\\code\\python\\causal_inference.py\n",
      "Function: sample_conditional at line 110\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   110                                               def sample_conditional(self, feature, parent_values):\n",
      "   111                                                   \"\"\"\n",
      "   112                                                   Sample a value for a feature conditioned on its parent features using precomputed regression model.\n",
      "   113                                                   \"\"\"\n",
      "   114    398850    6567327.0     16.5      1.2          effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
      "   115    148200     676373.0      4.6      0.1          if not effective_parents:\n",
      "   116                                                       return self.sample_marginal(feature)\n",
      "   117    148200    1983404.0     13.4      0.4          model_key = (feature, tuple(sorted(effective_parents))) \n",
      "   118    148200    1877936.0     12.7      0.3          if model_key not in self.regression_models:\n",
      "   119         7      59939.0   8562.7      0.0              X = self.data[effective_parents].values\n",
      "   120         7       8316.0   1188.0      0.0              y = self.data[feature].values\n",
      "   121         7        315.0     45.0      0.0              reg = LinearRegression()\n",
      "   122         7     185665.0  26523.6      0.0              reg.fit(X, y)\n",
      "   123         7      22556.0   3222.3      0.0              residuals = y - reg.predict(X)\n",
      "   124         7       5322.0    760.3      0.0              std = residuals.std()\n",
      "   125         7         76.0     10.9      0.0              self.regression_models[model_key] = (reg, std)\n",
      "   126    148200    1259523.0      8.5      0.2          reg, std = self.regression_models[model_key]\n",
      "   127    398850    7572987.0     19.0      1.4          parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
      "   128    148200  516745431.0   3486.8     94.6          mean = reg.predict(parent_values_array)[0]\n",
      "   129    148200    7251541.0     48.9      1.3          sampled_value = np.random.normal(mean, std)\n",
      "   130    148200    1955265.0     13.2      0.4          return sampled_value\n",
      "\n",
      "Total time: 1227.21 s\n",
      "File: c:\\Users\\snorl\\Desktop\\FYP\\code\\python\\causal_inference.py\n",
      "Function: compute_v_do at line 132\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   132                                               def compute_v_do(self, S, x_S, num_samples=50, is_classifier=False):\n",
      "   133       648       4655.0      7.2      0.0          samples = []\n",
      "   134     10368     163540.0     15.8      0.0          all_features = [col for col in self.data.columns if col != self.target_variable]\n",
      "   135       648    5732443.0   8846.4      0.0          variables_order = self.get_topological_order(S)\n",
      "   136     33048     309588.0      9.4      0.0          for _ in range(num_samples):\n",
      "   137     32400     700325.0     21.6      0.0              sample = {}\n",
      "   138    204800    1039531.0      5.1      0.0              for feature in S:\n",
      "   139    172400   35963531.0    208.6      0.3                  sample[feature] = x_S[feature]\n",
      "   140    550800    3355824.0      6.1      0.0              for feature in variables_order:\n",
      "   141    518400    3754652.0      7.2      0.0                  if feature in S or feature == self.target_variable:\n",
      "   142    204800     639733.0      3.1      0.0                      continue\n",
      "   143    313600   16428458.0     52.4      0.1                  parents = self.get_parents(feature)\n",
      "   144    564250    4779195.0      8.5      0.0                  parents = [p for p in parents if p != self.target_variable]\n",
      "   145    313600    1620887.0      5.2      0.0                  parent_values = {}\n",
      "   146    564250    2965853.0      5.3      0.0                  for parent in parents:\n",
      "   147    250650    1239045.0      4.9      0.0                      if parent in S:\n",
      "   148     71350   14795873.0    207.4      0.1                          parent_values[parent] = x_S[parent]\n",
      "   149                                                               else:\n",
      "   150    179300     921546.0      5.1      0.0                          parent_values[parent] = sample[parent]\n",
      "   151    313600    1504268.0      4.8      0.0                  if not parent_values:\n",
      "   152    165400  623817972.0   3771.6      5.1                      sample[feature] = self.sample_marginal(feature)\n",
      "   153                                                           else:\n",
      "   154    148200  558418799.0   3768.0      4.6                      sample[feature] = self.sample_conditional(feature, parent_values)\n",
      "   155    518400    2334469.0      4.5      0.0              for feature in all_features:\n",
      "   156    486000    2074301.0      4.3      0.0                  if feature not in sample and feature not in S:\n",
      "   157                                                               sample[feature] = self.sample_marginal(feature)\n",
      "   158     32400  386539941.0  11930.2      3.1              intervened_data = pd.DataFrame([sample])\n",
      "   159     32400  395421664.0  12204.4      3.2              intervened_data = intervened_data[self.model.feature_names_in_]\n",
      "   160     32400     221439.0      6.8      0.0              if is_classifier:\n",
      "   161                                                           proba = self.model.predict_proba(intervened_data)[0][1] \n",
      "   162                                                       else:\n",
      "   163     32400        1e+10 315015.7     83.2                  proba = self.model.predict(intervened_data)[0]\n",
      "   164     32400     332124.0     10.3      0.0              samples.append(proba)\n",
      "   165       648     526405.0    812.4      0.0          v_S = np.mean(samples)\n",
      "   166       648      13785.0     21.3      0.0          return v_S\n",
      "\n",
      "Total time: 1230.67 s\n",
      "File: c:\\Users\\snorl\\Desktop\\FYP\\code\\python\\causal_inference.py\n",
      "Function: compute_modified_shap_proba at line 168\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   168                                               def compute_modified_shap_proba(self, x, num_samples=50, shap_num_samples=50, is_classifier=False):\n",
      "   169        16        361.0     22.6      0.0          features = [col for col in self.data.columns if col != self.target_variable]\n",
      "   170         1         13.0     13.0      0.0          n_features = len(features)\n",
      "   171        16        172.0     10.8      0.0          phi_causal = {feature: 0.0 for feature in features}\n",
      "   172                                           \n",
      "   173         1      22427.0  22427.0      0.0          data_without_target = self.data.drop(columns=[self.target_variable], errors='ignore')\n",
      "   174         1      14437.0  14437.0      0.0          data_without_target = data_without_target[self.model.feature_names_in_]\n",
      "   175         1          6.0      6.0      0.0          if is_classifier:\n",
      "   176                                                       E_fX = self.model.predict_proba(data_without_target)[:, 1].mean() \n",
      "   177                                                   else:\n",
      "   178         1     491798.0 491798.0      0.0              E_fX = self.model.predict(data_without_target).mean()\n",
      "   179                                           \n",
      "   180         1      10794.0  10794.0      0.0          x_ordered = x[self.model.feature_names_in_]\n",
      "   181         1          6.0      6.0      0.0          if is_classifier:\n",
      "   182                                                       f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]  \n",
      "   183                                                   else:\n",
      "   184         1     324908.0 324908.0      0.0              f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
      "   185                                           \n",
      "   186        51        377.0      7.4      0.0          for _ in range(shap_num_samples):\n",
      "   187        50       8634.0    172.7      0.0              S_size = random.randint(0, n_features)\n",
      "   188        50      21312.0    426.2      0.0              S = random.sample(features, S_size)\n",
      "   189       800       5506.0      6.9      0.0              for i in features:\n",
      "   190       750       6173.0      8.2      0.0                  if i in S:\n",
      "   191       426       1268.0      3.0      0.0                      continue\n",
      "   192       324       4515.0     13.9      0.0                  S_without_i = S.copy()\n",
      "   193       324       3799.0     11.7      0.0                  S_with_i = S + [i]\n",
      "   194       324    3626018.0  11191.4      0.0                  x_S = x[S_without_i] if S_without_i else pd.Series(dtype=float)\n",
      "   195       324    2682153.0   8278.2      0.0                  x_Si = x[S_with_i] if S_with_i else pd.Series(dtype=float)\n",
      "   196       324 6174358714.0    2e+07     50.2                  v_S = self.compute_v_do(S_without_i, x_S, num_samples=num_samples, is_classifier=is_classifier)\n",
      "   197       324 6125103928.0    2e+07     49.8                  v_Si = self.compute_v_do(S_with_i, x_Si, num_samples=num_samples, is_classifier=is_classifier)\n",
      "   198       324      13583.0     41.9      0.0                  weight = (factorial(len(S_without_i)) * factorial(n_features - len(S_without_i) - 1)) / factorial(n_features)\n",
      "   199       324       5272.0     16.3      0.0                  gamma_i = self.gamma.get(i, 0.0)\n",
      "   200       324       1887.0      5.8      0.0                  weight *= gamma_i\n",
      "   201       324       3978.0     12.3      0.0                  delta_v = v_Si - v_S\n",
      "   202       324       8436.0     26.0      0.0                  phi_causal[i] += weight * delta_v\n",
      "   203                                           \n",
      "   204         1         44.0     44.0      0.0          sum_phi_causal = sum(phi_causal.values())\n",
      "   205         1         12.0     12.0      0.0          if sum_phi_causal == 0:\n",
      "   206                                                       phi_normalized = {k: 0.0 for k in phi_causal.keys()}\n",
      "   207                                                   else:\n",
      "   208         1         17.0     17.0      0.0              scaling_factor = (f_x - E_fX) / sum_phi_causal\n",
      "   209        16        116.0      7.2      0.0              phi_normalized = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
      "   210                                           \n",
      "   211         1         25.0     25.0      0.0          return phi_normalized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original Causal SHAP\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "ci = CausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(ci.compute_modified_shap_proba)\n",
    "profiler.add_function(ci.compute_v_do)\n",
    "profiler.add_function(ci.sample_conditional)\n",
    "\n",
    "profiler.run('phi_normalized = ci.compute_modified_shap_proba(x_instance)')\n",
    "profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Implementation of Compute_V_do with batch processing\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from causallearn.utils.cit import fisherz\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from math import factorial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class CausalInference:\n",
    "    def __init__(self, data, model, target_variable):\n",
    "        self.data = data  \n",
    "        self.pc_graph = None\n",
    "        self.model = model  \n",
    "        self.gamma = None  \n",
    "        self.target_variable = target_variable \n",
    "        self.ida_graph = None\n",
    "        self.regression_models = {} \n",
    "\n",
    "    def run_pc_algorithm(self, alpha=0.05):\n",
    "        data_np = self.data.to_numpy()\n",
    "        pc_result = pc(data_np, alpha, fisherz)\n",
    "        self.pc_graph = pc_result.G\n",
    "        return self.pc_graph\n",
    "\n",
    "    def draw_graph(self, file_path):\n",
    "        pyd = GraphUtils.to_pydot(self.pc_graph)\n",
    "        pyd.write_png(file_path)\n",
    "\n",
    "    def load_causal_strengths(self, json_file_path):\n",
    "        \"\"\"\n",
    "        Load causal strengths (beta_i) from JSON file and compute gamma_i.\n",
    "        \"\"\"\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            causal_effects_list = json.load(f)\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        nodes = list(self.data.columns)\n",
    "        G.add_nodes_from(nodes)\n",
    "\n",
    "        for item in causal_effects_list:\n",
    "            pair = item['Pair']\n",
    "            mean_causal_effect = item['Mean_Causal_Effect']\n",
    "            if mean_causal_effect is None:\n",
    "                continue  \n",
    "            source, target = pair.split('->')\n",
    "            source = source.strip()\n",
    "            target = target.strip()\n",
    "            G.add_edge(source, target, weight=mean_causal_effect)\n",
    "        self.ida_graph = G.copy()\n",
    "        features = self.data.columns.tolist()\n",
    "        beta_dict = {}\n",
    "\n",
    "        for feature in features:\n",
    "            if feature == self.target_variable:\n",
    "                continue\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(G, source=feature, target=self.target_variable))\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue  \n",
    "            total_effect = 0\n",
    "            for path in paths:\n",
    "                effect = 1\n",
    "                for i in range(len(path)-1):\n",
    "                    edge_weight = G[path[i]][path[i+1]]['weight']\n",
    "                    effect *= edge_weight\n",
    "                total_effect += effect\n",
    "            if total_effect != 0:\n",
    "                beta_dict[feature] = total_effect\n",
    "\n",
    "        total_causal_effect = sum(abs(beta) for beta in beta_dict.values())\n",
    "        if total_causal_effect == 0:\n",
    "            self.gamma = {k: 0.0 for k in features}\n",
    "        else:\n",
    "            self.gamma = {k: abs(beta_dict.get(k, 0.0)) / total_causal_effect for k in features}\n",
    "        return self.gamma\n",
    "    \n",
    "    def get_topological_order(self, S):\n",
    "        \"\"\"\n",
    "        Returns the topological order of variables after intervening on subset S.\n",
    "        \"\"\"\n",
    "        G_intervened = self.ida_graph.copy()\n",
    "        for feature in S:\n",
    "            G_intervened.remove_edges_from(list(G_intervened.in_edges(feature)))\n",
    "        missing_nodes = set(self.data.columns) - set(G_intervened.nodes)\n",
    "        G_intervened.add_nodes_from(missing_nodes)\n",
    "\n",
    "        try:\n",
    "            order = list(nx.topological_sort(G_intervened))\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            raise ValueError(\"The causal graph contains cycles.\")\n",
    "        \n",
    "        return order\n",
    "    \n",
    "    def get_parents(self, feature):\n",
    "        \"\"\"\n",
    "        Returns the list of parent features for a given feature in the causal graph.\n",
    "        \"\"\"\n",
    "        return list(self.ida_graph.predecessors(feature))\n",
    "\n",
    "    def sample_marginal(self, feature):\n",
    "        \"\"\"\n",
    "        Sample a value from the marginal distribution of the specified feature.\n",
    "        \"\"\"\n",
    "        return self.data[feature].sample(1).iloc[0]\n",
    "\n",
    "    def sample_conditional(self, feature, parent_values):\n",
    "        \"\"\"\n",
    "        Sample a value for a feature conditioned on its parent features using precomputed regression model.\n",
    "        \"\"\"\n",
    "        effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
    "        if not effective_parents:\n",
    "            return self.sample_marginal(feature)\n",
    "        model_key = (feature, tuple(sorted(effective_parents))) \n",
    "        if model_key not in self.regression_models:\n",
    "            X = self.data[effective_parents].values\n",
    "            y = self.data[feature].values\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "            residuals = y - reg.predict(X)\n",
    "            std = residuals.std()\n",
    "            self.regression_models[model_key] = (reg, std)\n",
    "        reg, std = self.regression_models[model_key]\n",
    "        parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
    "        mean = reg.predict(parent_values_array)[0]\n",
    "        sampled_value = np.random.normal(mean, std)\n",
    "        return sampled_value\n",
    "\n",
    "    def compute_v_do(self, S, x_S, num_samples=50, is_classifier=False):\n",
    "        samples_list = []\n",
    "        all_features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        variables_order = self.get_topological_order(S)\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            sample = {}\n",
    "            for feature in S:\n",
    "                sample[feature] = x_S[feature]\n",
    "            for feature in variables_order:\n",
    "                if feature in S or feature == self.target_variable:\n",
    "                    continue\n",
    "                parents = self.get_parents(feature)\n",
    "                parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
    "                if not parent_values:\n",
    "                    sample[feature] = self.sample_marginal(feature)\n",
    "                else:\n",
    "                    sample[feature] = self.sample_conditional(feature, parent_values)\n",
    "            samples_list.append(sample)\n",
    "        \n",
    "        intervened_data = pd.DataFrame(samples_list)\n",
    "        intervened_data = intervened_data[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            probas = self.model.predict_proba(intervened_data)[:, 1]\n",
    "        else:\n",
    "            probas = self.model.predict(intervened_data)\n",
    "        \n",
    "        return np.mean(probas)\n",
    "\n",
    "    def compute_modified_shap_proba(self, x, num_samples=50, shap_num_samples=50, is_classifier=False):\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        n_features = len(features)\n",
    "        phi_causal = {feature: 0.0 for feature in features}\n",
    "\n",
    "        data_without_target = self.data.drop(columns=[self.target_variable], errors='ignore')\n",
    "        data_without_target = data_without_target[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            E_fX = self.model.predict_proba(data_without_target)[:, 1].mean() \n",
    "        else:\n",
    "            E_fX = self.model.predict(data_without_target).mean()\n",
    "\n",
    "        x_ordered = x[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]  \n",
    "        else:\n",
    "            f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
    "\n",
    "        for _ in range(shap_num_samples):\n",
    "            S_size = random.randint(0, n_features)\n",
    "            S = random.sample(features, S_size)\n",
    "            for i in features:\n",
    "                if i in S:\n",
    "                    continue\n",
    "                S_without_i = S.copy()\n",
    "                S_with_i = S + [i]\n",
    "                x_S = x[S_without_i] if S_without_i else pd.Series(dtype=float)\n",
    "                x_Si = x[S_with_i] if S_with_i else pd.Series(dtype=float)\n",
    "                v_S = self.compute_v_do(S_without_i, x_S, num_samples=num_samples, is_classifier=is_classifier)\n",
    "                v_Si = self.compute_v_do(S_with_i, x_Si, num_samples=num_samples, is_classifier=is_classifier)\n",
    "                weight = (factorial(len(S_without_i)) * factorial(n_features - len(S_without_i) - 1)) / factorial(n_features)\n",
    "                gamma_i = self.gamma.get(i, 0.0)\n",
    "                weight *= gamma_i\n",
    "                delta_v = v_Si - v_S\n",
    "                phi_causal[i] += weight * delta_v\n",
    "\n",
    "        sum_phi_causal = sum(phi_causal.values())\n",
    "        if sum_phi_causal == 0:\n",
    "            phi_normalized = {k: 0.0 for k in phi_causal.keys()}\n",
    "        else:\n",
    "            scaling_factor = (f_x - E_fX) / sum_phi_causal\n",
    "            phi_normalized = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
    "\n",
    "        return phi_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 50.9047 s\n",
      "File: C:\\Users\\snorl\\AppData\\Local\\Temp\\ipykernel_35000\\1627813417.py\n",
      "Function: sample_conditional at line 110\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   110                                               def sample_conditional(self, feature, parent_values):\n",
      "   111                                                   \"\"\"\n",
      "   112                                                   Sample a value for a feature conditioned on its parent features using precomputed regression model.\n",
      "   113                                                   \"\"\"\n",
      "   114    392650    6407104.0     16.3      1.3          effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
      "   115    146200     615117.0      4.2      0.1          if not effective_parents:\n",
      "   116                                                       return self.sample_marginal(feature)\n",
      "   117    146200    1723889.0     11.8      0.3          model_key = (feature, tuple(sorted(effective_parents))) \n",
      "   118    146200    1415815.0      9.7      0.3          if model_key not in self.regression_models:\n",
      "   119         7      84731.0  12104.4      0.0              X = self.data[effective_parents].values\n",
      "   120         7       3235.0    462.1      0.0              y = self.data[feature].values\n",
      "   121         7       1322.0    188.9      0.0              reg = LinearRegression()\n",
      "   122         7     254015.0  36287.9      0.0              reg.fit(X, y)\n",
      "   123         7      38288.0   5469.7      0.0              residuals = y - reg.predict(X)\n",
      "   124         7      10873.0   1553.3      0.0              std = residuals.std()\n",
      "   125         7        117.0     16.7      0.0              self.regression_models[model_key] = (reg, std)\n",
      "   126    146200    1032695.0      7.1      0.2          reg, std = self.regression_models[model_key]\n",
      "   127    392650    7199215.0     18.3      1.4          parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
      "   128    146200  482860949.0   3302.7     94.9          mean = reg.predict(parent_values_array)[0]\n",
      "   129    146200    5585820.0     38.2      1.1          sampled_value = np.random.normal(mean, std)\n",
      "   130    146200    1813801.0     12.4      0.4          return sampled_value\n",
      "\n",
      "Total time: 135.097 s\n",
      "File: C:\\Users\\snorl\\AppData\\Local\\Temp\\ipykernel_35000\\1627813417.py\n",
      "Function: compute_v_do at line 132\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   132                                               def compute_v_do(self, S, x_S, num_samples=50, is_classifier=False):\n",
      "   133       668       4912.0      7.4      0.0          samples_list = []\n",
      "   134     10688     177846.0     16.6      0.0          all_features = [col for col in self.data.columns if col != self.target_variable]\n",
      "   135       668    5093812.0   7625.5      0.4          variables_order = self.get_topological_order(S)\n",
      "   136                                                   \n",
      "   137     34068     214174.0      6.3      0.0          for _ in range(num_samples):\n",
      "   138     33400     140131.0      4.2      0.0              sample = {}\n",
      "   139    228900    1019528.0      4.5      0.1              for feature in S:\n",
      "   140    195500   29800911.0    152.4      2.2                  sample[feature] = x_S[feature]\n",
      "   141    567800    3160442.0      5.6      0.2              for feature in variables_order:\n",
      "   142    534400    3301832.0      6.2      0.2                  if feature in S or feature == self.target_variable:\n",
      "   143    228900     725270.0      3.2      0.1                      continue\n",
      "   144    305500   12572731.0     41.2      0.9                  parents = self.get_parents(feature)\n",
      "   145    551950   19458412.0     35.3      1.4                  parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
      "   146    305500    1393408.0      4.6      0.1                  if not parent_values:\n",
      "   147    159300  516202196.0   3240.4     38.2                      sample[feature] = self.sample_marginal(feature)\n",
      "   148                                                           else:\n",
      "   149    146200  519921568.0   3556.2     38.5                      sample[feature] = self.sample_conditional(feature, parent_values)\n",
      "   150     33400     195121.0      5.8      0.0              samples_list.append(sample)\n",
      "   151                                                   \n",
      "   152       668    9581112.0  14343.0      0.7          intervened_data = pd.DataFrame(samples_list)\n",
      "   153       668    8376878.0  12540.2      0.6          intervened_data = intervened_data[self.model.feature_names_in_]\n",
      "   154       668       4496.0      6.7      0.0          if is_classifier:\n",
      "   155                                                       probas = self.model.predict_proba(intervened_data)[:, 1]\n",
      "   156                                                   else:\n",
      "   157       668  219215539.0 328167.0     16.2              probas = self.model.predict(intervened_data)\n",
      "   158                                                   \n",
      "   159       668     412940.0    618.2      0.0          return np.mean(probas)\n",
      "\n",
      "Total time: 137.262 s\n",
      "File: C:\\Users\\snorl\\AppData\\Local\\Temp\\ipykernel_35000\\1627813417.py\n",
      "Function: compute_modified_shap_proba at line 161\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   161                                               def compute_modified_shap_proba(self, x, num_samples=50, shap_num_samples=50, is_classifier=False):\n",
      "   162        16        318.0     19.9      0.0          features = [col for col in self.data.columns if col != self.target_variable]\n",
      "   163         1          7.0      7.0      0.0          n_features = len(features)\n",
      "   164        16         71.0      4.4      0.0          phi_causal = {feature: 0.0 for feature in features}\n",
      "   165                                           \n",
      "   166         1      24057.0  24057.0      0.0          data_without_target = self.data.drop(columns=[self.target_variable], errors='ignore')\n",
      "   167         1      10061.0  10061.0      0.0          data_without_target = data_without_target[self.model.feature_names_in_]\n",
      "   168         1          6.0      6.0      0.0          if is_classifier:\n",
      "   169                                                       E_fX = self.model.predict_proba(data_without_target)[:, 1].mean() \n",
      "   170                                                   else:\n",
      "   171         1     319653.0 319653.0      0.0              E_fX = self.model.predict(data_without_target).mean()\n",
      "   172                                           \n",
      "   173         1       8830.0   8830.0      0.0          x_ordered = x[self.model.feature_names_in_]\n",
      "   174         1          5.0      5.0      0.0          if is_classifier:\n",
      "   175                                                       f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]  \n",
      "   176                                                   else:\n",
      "   177         1     314156.0 314156.0      0.0              f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
      "   178                                           \n",
      "   179        51        377.0      7.4      0.0          for _ in range(shap_num_samples):\n",
      "   180        50       8337.0    166.7      0.0              S_size = random.randint(0, n_features)\n",
      "   181        50      21933.0    438.7      0.0              S = random.sample(features, S_size)\n",
      "   182       800       5956.0      7.4      0.0              for i in features:\n",
      "   183       750       6024.0      8.0      0.0                  if i in S:\n",
      "   184       416       1229.0      3.0      0.0                      continue\n",
      "   185       334       4485.0     13.4      0.0                  S_without_i = S.copy()\n",
      "   186       334       3640.0     10.9      0.0                  S_with_i = S + [i]\n",
      "   187       334    3766550.0  11277.1      0.3                  x_S = x[S_without_i] if S_without_i else pd.Series(dtype=float)\n",
      "   188       334    2900217.0   8683.3      0.2                  x_Si = x[S_with_i] if S_with_i else pd.Series(dtype=float)\n",
      "   189       334  709868796.0    2e+06     51.7                  v_S = self.compute_v_do(S_without_i, x_S, num_samples=num_samples, is_classifier=is_classifier)\n",
      "   190       334  655316709.0    2e+06     47.7                  v_Si = self.compute_v_do(S_with_i, x_Si, num_samples=num_samples, is_classifier=is_classifier)\n",
      "   191       334      13369.0     40.0      0.0                  weight = (factorial(len(S_without_i)) * factorial(n_features - len(S_without_i) - 1)) / factorial(n_features)\n",
      "   192       334       4586.0     13.7      0.0                  gamma_i = self.gamma.get(i, 0.0)\n",
      "   193       334       2259.0      6.8      0.0                  weight *= gamma_i\n",
      "   194       334       5062.0     15.2      0.0                  delta_v = v_Si - v_S\n",
      "   195       334       8299.0     24.8      0.0                  phi_causal[i] += weight * delta_v\n",
      "   196                                           \n",
      "   197         1         68.0     68.0      0.0          sum_phi_causal = sum(phi_causal.values())\n",
      "   198         1         10.0     10.0      0.0          if sum_phi_causal == 0:\n",
      "   199                                                       phi_normalized = {k: 0.0 for k in phi_causal.keys()}\n",
      "   200                                                   else:\n",
      "   201         1         16.0     16.0      0.0              scaling_factor = (f_x - E_fX) / sum_phi_causal\n",
      "   202        16        116.0      7.2      0.0              phi_normalized = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
      "   203                                           \n",
      "   204         1         24.0     24.0      0.0          return phi_normalized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original Causal SHAP\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "ci = CausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(ci.compute_modified_shap_proba)\n",
    "profiler.add_function(ci.compute_v_do)\n",
    "profiler.add_function(ci.sample_conditional)\n",
    "\n",
    "profiler.run('phi_normalized = ci.compute_modified_shap_proba(x_instance)')\n",
    "profiler.print_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
