{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "DEBUG:matplotlib:matplotlib data path: c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\matplotlib\\mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=C:\\Users\\snorl\\.matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is win32\n",
      "DEBUG:matplotlib:CACHEDIR=C:\\Users\\snorl\\.matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from C:\\Users\\snorl\\.matplotlib\\fontlist-v390.json\n",
      "DEBUG:pydot:pydot initializing\n",
      "DEBUG:pydot:pydot 3.0.2\n",
      "DEBUG:pydot.dot_parser:pydot dot_parser module initializing\n",
      "DEBUG:pydot.core:pydot core module initializing\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from data_processing import DataProcessor\n",
    "from models import ModelTrainer\n",
    "from feature_selection import FeatureSelector\n",
    "from visualization import Visualizer\n",
    "from causal_inference import CausalInference\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging \n",
    "log = logging.getLogger('shap')\n",
    "log.setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML Pipeline...\n",
      "Base directory set to: ../../../\n",
      "Loading data...\n",
      "Data loaded successfully.\n",
      "Preprocessing raw data...\n",
      "Raw data preprocessed successfully.\n",
      "Encoding labels...\n",
      "Labels encoded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting ML Pipeline...\")\n",
    "base_dir = '../../../'\n",
    "print(f\"Base directory set to: {base_dir}\")\n",
    "\n",
    "data_path = base_dir + 'dataset/' + 'data_full.xlsx'\n",
    "raw_data_path = base_dir + 'dataset/' + 'result_raw.xlsx'\n",
    "result_dir = base_dir + 'result/R/'\n",
    "\n",
    "report_file_path = result_dir + 'report.txt'\n",
    "\n",
    "print(\"Loading data...\")\n",
    "data_processor = DataProcessor(data_path=str(data_path))\n",
    "df = data_processor.load_data_metabolites()\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"Preprocessing raw data...\")\n",
    "raw_df = data_processor.preprocess_raw_data(raw_data_path=str(raw_data_path))\n",
    "print(\"Raw data preprocessed successfully.\")\n",
    "\n",
    "print(\"Encoding labels...\")\n",
    "df_encoded, label_encoder = data_processor.encode_labels(df, label_column='Group')\n",
    "print(\"Labels encoded successfully.\")\n",
    "\n",
    "X = df_encoded.drop(columns=['Group'])\n",
    "y = df_encoded['Group']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "X = X[[\"xylose\", \"xanthosine\", \"uracil\", \"ribulose/xylulose\", \"valylglutamine\", \"tryptophylglycine\", \"succinate\", \"valine betaine\", \"ursodeoxycholate sulfate (1)\", \"tricarballylate\",\"succinimide\", \"thymine\", \"syringic acid\", \"serotonin\", \"ribitol\" ]]\n",
    "\n",
    "y = df_encoded['Group']\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 7],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "model_trainer = ModelTrainer(X, y)\n",
    "model, best_params = model_trainer.train_random_forest(param_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "0.8108108108108109\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71        26\n",
      "           1       0.83      0.90      0.86        48\n",
      "\n",
      "    accuracy                           0.81        74\n",
      "   macro avg       0.80      0.77      0.78        74\n",
      "weighted avg       0.81      0.81      0.81        74\n",
      "\n",
      "Model evaluation completed.\n",
      "Saving trained model...\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model...\")\n",
    "accuracy, report = model_trainer.evaluate_model()\n",
    "print(accuracy)\n",
    "print(report)\n",
    "print(\"Model evaluation completed.\")\n",
    "\n",
    "print(\"Saving trained model...\")\n",
    "model_trainer.save_model(str(result_dir + 'best_random_forest_model.pkl'))\n",
    "print(\"Model saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causal_inference import CausalInference\n",
    "from evaluation import iterative_feature_addition_with_rmse, iterative_feature_deletion_with_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying causal SHAP values...\n",
      "{'xylose': 0.21286483817631863, 'xanthosine': 0.02394692431616487, 'uracil': 0.09473882478767023, 'ribulose/xylulose': 0.1846557343833871, 'valylglutamine': 0.006014187808621013, 'tryptophylglycine': 0.010263874030019977, 'succinate': 0.05945970043865379, 'valine betaine': 0.11760414785893997, 'ursodeoxycholate sulfate (1)': 0.0019060014073747102, 'tricarballylate': 0.0036239283407113083, 'succinimide': 0.018407825414330586, 'thymine': 0.08182038074786072, 'syringic acid': 0.004553557947259814, 'serotonin': 0.1350346545925681, 'ribitol': 0.0451054197501192}\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying causal SHAP values...\")\n",
    "ci = CausalInference(data=model_trainer.X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "print(ci.gamma)\n",
    "X_train_scaled_df = pd.DataFrame(model_trainer.X_train, columns=model_trainer.X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(model_trainer.X_test, columns=model_trainer.X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Causal SHAP for explainability...\n",
      "Applying Kernel SHAP for explainability...\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying Causal SHAP for explainability...\")\n",
    "import shap\n",
    "\n",
    "background_data = shap.kmeans(model_trainer.X_train, 100)\n",
    "explainer = shap.CausalKernelExplainer(model.predict_proba, background_data, ci.ida_graph, ci.gamma, feature_names=model_trainer.X_train.columns.tolist())\n",
    "\n",
    "causal_shap_values = explainer.shap_values(model_trainer.X_test.iloc[10])\n",
    "\n",
    "print(\"Applying Kernel SHAP for explainability...\")\n",
    "\n",
    "# Use KMeans background data for SHAP Kernel Explainer\n",
    "explainer = shap.KernelExplainer(model.predict_proba, background_data)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(model_trainer.X_test.iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08872605  0.08872605]\n",
      " [ 0.02983786 -0.02983786]\n",
      " [ 0.          0.        ]\n",
      " [-0.17084405  0.17084405]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [-0.04535461  0.04535461]\n",
      " [ 0.00796015 -0.00796015]\n",
      " [ 0.01311511 -0.01311511]\n",
      " [ 0.02841278 -0.02841278]\n",
      " [-0.04633111  0.04633111]\n",
      " [-0.02035613  0.02035613]\n",
      " [-0.00575002  0.00575002]\n",
      " [ 0.10378905 -0.10378905]\n",
      " [ 0.04569806 -0.04569806]]\n"
     ]
    }
   ],
   "source": [
    "print(causal_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m global_shap_causal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(causal_shap_values)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for better readability and sorting\u001b[39;00m\n\u001b[0;32m      8\u001b[0m feature_importance_causal \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: model_trainer\u001b[38;5;241m.\u001b[39mX_train\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Absolute SHAP Value\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mglobal_shap_causal\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m })\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Sort features by importance in descending order\u001b[39;00m\n\u001b[0;32m     14\u001b[0m feature_importance_causal \u001b[38;5;241m=\u001b[39m feature_importance_causal\u001b[38;5;241m.\u001b[39msort_values(\n\u001b[0;32m     15\u001b[0m     by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Absolute SHAP Value\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Compute mean absolute SHAP values for each feature across all samples\n",
    "global_shap_causal = np.abs(causal_shap_values).mean(axis=0)\n",
    "\n",
    "# Create a DataFrame for better readability and sorting\n",
    "feature_importance_causal = pd.DataFrame({\n",
    "    'Feature': model_trainer.X_train.columns,\n",
    "    'Mean Absolute SHAP Value': global_shap_causal[:, 1]\n",
    "})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_causal = feature_importance_causal.sort_values(\n",
    "    by='Mean Absolute SHAP Value', ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Print the global SHAP values\n",
    "print(\"Global SHAP Values from CausalKernelExplainer:\")\n",
    "print(feature_importance_causal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global SHAP Values from KernelExplainer:\n",
      "                         Feature  Mean Absolute SHAP Value\n",
      "0                         xylose                  0.124013\n",
      "1                      serotonin                  0.064839\n",
      "2              ribulose/xylulose                  0.050595\n",
      "3                 valine betaine                  0.034927\n",
      "4                      succinate                  0.024271\n",
      "5                        ribitol                  0.015648\n",
      "6              tryptophylglycine                  0.013389\n",
      "7                tricarballylate                  0.012819\n",
      "8   ursodeoxycholate sulfate (1)                  0.012815\n",
      "9                     xanthosine                  0.012612\n",
      "10                   succinimide                  0.012147\n",
      "11                       thymine                  0.009973\n",
      "12                valylglutamine                  0.008555\n",
      "13                        uracil                  0.005946\n",
      "14                 syringic acid                  0.005843\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Compute mean absolute SHAP values for each feature across all samples\n",
    "global_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create a DataFrame for better readability and sorting\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': model_trainer.X_train.columns,\n",
    "    'Mean Absolute SHAP Value': global_shap[:, 1]\n",
    "})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance = feature_importance.sort_values(\n",
    "    by='Mean Absolute SHAP Value', ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Print the global SHAP values\n",
    "print(\"Global SHAP Values from KernelExplainer:\")\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14854895456343487\n",
      "0.14854895456343487\n",
      "Instance: 10\n",
      "Instance Predicted Value: [1]\n",
      "Comparing SHAP and Causal SHAP using feature deletion and addition...\n",
      "Average RMSE (Standard SHAP - Deletion): 0.1976272264772264\n",
      "Average RMSE (Causal SHAP - Deletion): 0.1795235028062613\n",
      "Average RMSE (Standard SHAP - Addition): 0.23733154367981962\n",
      "Average RMSE (Causal SHAP - Addition): 0.2495047826375412\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "print(sum(causal_shap_values[:,1]))\n",
    "print(sum(shap_values[:,1]))\n",
    "\n",
    "\n",
    "x_instance = pd.Series(X_test_scaled_df.iloc[i], index=X_test_scaled_df.columns)\n",
    "# Compare standard SHAP and causal SHAP values\n",
    "standard_shap_series = pd.Series(shap_values[:,1], index=X_test_scaled_df.columns)\n",
    "causal_shap_series = pd.Series(causal_shap_values[:,1], index=X_test_scaled_df.columns)\n",
    "y_predicted = model.predict(X_test_scaled_df.iloc[i].values.reshape(1, -1))\n",
    "print(f\"Instance: {i}\")\n",
    "print(f\"Instance Predicted Value: {y_predicted}\")\n",
    "\n",
    "# Apply iterative feature deletion and addition to compare SHAP values\n",
    "print(\"Comparing SHAP and Causal SHAP using feature deletion and addition...\")\n",
    "avg_output_standard_deletion = iterative_feature_deletion_with_rmse(\n",
    "    model=model, input_features=x_instance, attribution_scores=standard_shap_series, y_predicted=y_predicted)\n",
    "\n",
    "avg_output_causal_deletion = iterative_feature_deletion_with_rmse(\n",
    "    model=model, input_features=x_instance, attribution_scores=causal_shap_series, y_predicted=y_predicted)\n",
    "\n",
    "avg_output_standard_addition = iterative_feature_addition_with_rmse(\n",
    "    model=model, input_features=x_instance, attribution_scores=standard_shap_series, y_predicted=y_predicted)\n",
    "\n",
    "avg_output_causal_addition = iterative_feature_addition_with_rmse(\n",
    "    model=model, input_features=x_instance, attribution_scores=causal_shap_series, y_predicted=y_predicted)\n",
    "\n",
    "print(f\"Average RMSE (Standard SHAP - Deletion): {avg_output_standard_deletion}\")\n",
    "print(f\"Average RMSE (Causal SHAP - Deletion): {avg_output_causal_deletion}\")\n",
    "print(f\"Average RMSE (Standard SHAP - Addition): {avg_output_standard_addition}\")\n",
    "print(f\"Average RMSE (Causal SHAP - Addition): {avg_output_causal_addition}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
