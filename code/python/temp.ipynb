{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fast_causal_inference import FastCausalInference\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../../'\n",
    "result_dir = base_dir + 'result/R/'\n",
    "data_path = base_dir + 'dataset/' + 'Real_World_IBS.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "df = df.drop(columns=['HAD_Anxiety', 'Patient', 'Batch_metabolomics', 'BH', 'Sex', 'Age', 'BMI','Race','Education','HAD_Depression','STAI_Tanxiety', 'Diet_Category','Diet_Pattern'])\n",
    "label_encoder = LabelEncoder()\n",
    "df['Group'] = label_encoder.fit_transform(df['Group'])\n",
    "df_encoded = df\n",
    "\n",
    "X = df_encoded.drop(columns=['Group'])\n",
    "y = df_encoded['Group']\n",
    "\n",
    "X = X[[\"xylose\", \"xanthosine\", \"uracil\", \"ribulose/xylulose\", \"valylglutamine\",\n",
    "           \"tryptophylglycine\", \"succinate\", \"valine betaine\", \"ursodeoxycholate sulfate (1)\",\n",
    "           \"tricarballylate\", \"succinimide\", \"thymine\", \"syringic acid\", \"serotonin\", \"ribitol\"]]\n",
    "\n",
    "param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 7],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "estimator=rf, param_distributions=param_dist, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../'\n",
    "data_path = base_dir + 'dataset/' + 'Real_World_IBS_Predicted_Probabilities.xlsx'\n",
    "df_prob = pd.read_excel(data_path)\n",
    "X_train['Prob_Class_1'] = df_prob['Prob_Class_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "from math import factorial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n",
    "class FastCausalInference:\n",
    "    def __init__(self, data, model, target_variable):\n",
    "        self.data = data  \n",
    "        self.model = model  \n",
    "        self.gamma = None  \n",
    "        self.target_variable = target_variable \n",
    "        self.ida_graph = None\n",
    "        self.regression_models = {}\n",
    "        self.feature_depths = {}\n",
    "        self.path_cache = {}\n",
    "        self.causal_paths = {}  \n",
    "        \n",
    "    def _compute_causal_paths(self):\n",
    "        \"\"\"Compute and store all causal paths to target for each feature.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        for feature in features:\n",
    "            try:\n",
    "                # Store the actual paths instead of just the features\n",
    "                paths = list(nx.all_simple_paths(self.ida_graph, feature, self.target_variable))\n",
    "                self.causal_paths[feature] = paths\n",
    "            except nx.NetworkXNoPath:\n",
    "                self.causal_paths[feature] = []\n",
    "\n",
    "    def load_causal_strengths(self, json_file_path):\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            causal_effects_list = json.load(f)\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        nodes = list(self.data.columns)\n",
    "        G.add_nodes_from(nodes)\n",
    "\n",
    "        for item in causal_effects_list:\n",
    "            pair = item['Pair']\n",
    "            mean_causal_effect = item['Mean_Causal_Effect']\n",
    "            if mean_causal_effect is None:\n",
    "                continue  \n",
    "            source, target = pair.split('->')\n",
    "            source = source.strip()\n",
    "            target = target.strip()\n",
    "            G.add_edge(source, target, weight=mean_causal_effect)\n",
    "        self.ida_graph = G.copy()\n",
    "        self._compute_feature_depths()\n",
    "        self._compute_causal_paths()\n",
    "        features = self.data.columns.tolist()\n",
    "        beta_dict = {}\n",
    "\n",
    "        for feature in features:\n",
    "            if feature == self.target_variable:\n",
    "                continue\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(G, source=feature, target=self.target_variable))\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue  \n",
    "            total_effect = 0\n",
    "            for path in paths:\n",
    "                effect = 1\n",
    "                for i in range(len(path)-1):\n",
    "                    edge_weight = G[path[i]][path[i+1]]['weight']\n",
    "                    effect *= edge_weight\n",
    "                total_effect += effect\n",
    "            if total_effect != 0:\n",
    "                beta_dict[feature] = total_effect\n",
    "\n",
    "        total_causal_effect = sum(abs(beta) for beta in beta_dict.values())\n",
    "        if total_causal_effect == 0:\n",
    "            self.gamma = {k: 0.0 for k in features}\n",
    "        else:\n",
    "            self.gamma = {k: abs(beta_dict.get(k, 0.0)) / total_causal_effect for k in features}\n",
    "        return self.gamma\n",
    "    \n",
    "    def _compute_feature_depths(self):\n",
    "        \"\"\"Compute minimum depth of each feature to target in causal graph.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        for feature in features:\n",
    "            try:\n",
    "                all_paths = list(nx.all_simple_paths(self.ida_graph, feature, self.target_variable))\n",
    "                min_depth = float('inf')\n",
    "                for path in all_paths:\n",
    "                    depth = len(path) - 1  \n",
    "                    min_depth = min(min_depth, depth)\n",
    "                if min_depth != float('inf'):\n",
    "                    self.feature_depths[feature] = min_depth\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    def get_topological_order(self, S):\n",
    "        \"\"\"Returns the topological order of variables after intervening on subset S.\"\"\"\n",
    "        G_intervened = self.ida_graph.copy()\n",
    "        for feature in S:\n",
    "            G_intervened.remove_edges_from(list(G_intervened.in_edges(feature)))\n",
    "        missing_nodes = set(self.data.columns) - set(G_intervened.nodes)\n",
    "        G_intervened.add_nodes_from(missing_nodes)\n",
    "\n",
    "        try:\n",
    "            order = list(nx.topological_sort(G_intervened))\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            raise ValueError(\"The causal graph contains cycles.\")\n",
    "        \n",
    "        return order\n",
    "    \n",
    "    def get_parents(self, feature):\n",
    "        \"\"\"Returns the list of parent features for a given feature in the causal graph.\"\"\"\n",
    "        return list(self.ida_graph.predecessors(feature))\n",
    "\n",
    "    def sample_marginal(self, feature):\n",
    "        \"\"\"Sample a value from the marginal distribution of the specified feature.\"\"\"\n",
    "        return self.data[feature].sample(1).iloc[0]\n",
    "\n",
    "    def sample_conditional(self, feature, parent_values):\n",
    "        \"\"\"Sample a value for a feature conditioned on its parent features.\"\"\"\n",
    "        effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
    "        if not effective_parents:\n",
    "            return self.sample_marginal(feature)\n",
    "        model_key = (feature, tuple(sorted(effective_parents))) \n",
    "        if model_key not in self.regression_models:\n",
    "            X = self.data[effective_parents].values\n",
    "            y = self.data[feature].values\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "            residuals = y - reg.predict(X)\n",
    "            std = residuals.std()\n",
    "            self.regression_models[model_key] = (reg, std)\n",
    "        reg, std = self.regression_models[model_key]\n",
    "        parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
    "        mean = reg.predict(parent_values_array)[0]\n",
    "        sampled_value = np.random.normal(mean, std)\n",
    "        return sampled_value\n",
    "\n",
    "    def compute_v_do(self, S, x_S, is_classifier=False):\n",
    "        \"\"\"Compute interventional expectations with caching.\"\"\"\n",
    "        cache_key = (frozenset(S), tuple(sorted(x_S.items())) if len(x_S) > 0 else tuple())\n",
    "        \n",
    "        if cache_key in self.path_cache:\n",
    "            return self.path_cache[cache_key]\n",
    "        \n",
    "        variables_order = self.get_topological_order(S)\n",
    "        \n",
    "        sample = {}\n",
    "        for feature in S:\n",
    "            sample[feature] = x_S[feature]\n",
    "        for feature in variables_order:\n",
    "            if feature in S or feature == self.target_variable:\n",
    "                continue\n",
    "            parents = self.get_parents(feature)\n",
    "            parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
    "            if not parent_values:\n",
    "                sample[feature] = self.sample_marginal(feature)\n",
    "            else:\n",
    "                sample[feature] = self.sample_conditional(feature, parent_values)\n",
    "               \n",
    "        intervened_data = pd.DataFrame([sample])\n",
    "        intervened_data = intervened_data[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            probas = self.model.predict_proba(intervened_data)[:, 1]\n",
    "        else:\n",
    "            probas = self.model.predict(intervened_data)\n",
    "        \n",
    "        result = np.mean(probas)\n",
    "        self.path_cache[cache_key] = result\n",
    "        return result\n",
    "\n",
    "    def is_on_causal_path(self, feature, S, target_feature):\n",
    "        \"\"\"Check if feature is on any causal path from S to target_feature.\"\"\"\n",
    "        if target_feature not in self.causal_paths:\n",
    "            return False\n",
    "        path_features = self.causal_paths[target_feature]\n",
    "        return feature in path_features\n",
    "\n",
    "    def compute_modified_shap_proba(self, x, is_classifier=False):\n",
    "        \"\"\"TreeSHAP-inspired computation using causal paths and dynamic programming.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        phi_causal = {feature: 0.0 for feature in features}\n",
    "\n",
    "        data_without_target = self.data.drop(columns=[self.target_variable])\n",
    "        if is_classifier:\n",
    "            E_fX = self.model.predict_proba(data_without_target)[:, 1].mean()\n",
    "        else:\n",
    "            E_fX = self.model.predict(data_without_target).mean()\n",
    "\n",
    "        x_ordered = x[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]\n",
    "        else:\n",
    "            f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
    "\n",
    "        sorted_features = sorted(features, key=lambda f: self.feature_depths.get(f, 0))\n",
    "        max_path_length = max(self.feature_depths.values(), default=0)\n",
    "        shapley_weights = {}\n",
    "        for m in range(max_path_length + 1):\n",
    "            for d in range(m + 1, max_path_length + 1):\n",
    "                shapley_weights[(m, d)] = (factorial(m) * factorial(d - m - 1)) / factorial(d)\n",
    "\n",
    "        # Track contributions using dynamic programming (EXTEND-like logic in TreeSHAP)\n",
    "        # m_values will accumulate contributions from subsets (use combinatorial logic)\n",
    "        # Essentially, values in m_values[k] represent how many ways there are to select k nodes from the path seen so far.\n",
    "        for feature in sorted_features:\n",
    "            if feature not in self.causal_paths:\n",
    "                continue\n",
    "            for path in self.causal_paths[feature]:\n",
    "                path_features = [n for n in path if n != self.target_variable]\n",
    "                d = len(path_features)\n",
    "                m_values = defaultdict(float)\n",
    "                m_values[0] = 1.0\n",
    "\n",
    "                for node in path_features:\n",
    "                    if node == feature:\n",
    "                        continue  \n",
    "\n",
    "                    new_m_values = defaultdict(float)\n",
    "                    for m, val in m_values.items():\n",
    "                        new_m_values[m + 1] += val \n",
    "                        new_m_values[m] += val \n",
    "                    m_values = new_m_values\n",
    "\n",
    "                for m in m_values:\n",
    "                    weight = shapley_weights.get((m, d), 0) * self.gamma.get(feature, 0)\n",
    "                    delta_v = self._compute_path_delta_v(feature, path, m, x, is_classifier)\n",
    "                    phi_causal[feature] += weight * delta_v * m_values[m]\n",
    "\n",
    "        sum_phi = sum(phi_causal.values())\n",
    "        if sum_phi != 0:\n",
    "            scaling_factor = (f_x - E_fX) / sum_phi\n",
    "            phi_causal = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
    "\n",
    "        return phi_causal\n",
    "        \n",
    "    def _compute_path_delta_v(self, feature, path, m, x, is_classifier):\n",
    "        \"\"\"Compute Î”v for a causal path using precomputed expectations.\"\"\"\n",
    "        S = [n for n in path[:m] if n != feature]\n",
    "        x_S = {n: x[n] for n in S if n in x}\n",
    "        v_S = self.compute_v_do(S, x_S, is_classifier)\n",
    "\n",
    "        S_with_i = S + [feature]\n",
    "        x_Si = {**x_S, feature: x[feature]}\n",
    "        v_Si = self.compute_v_do(S_with_i, x_Si, is_classifier)\n",
    "\n",
    "        return v_Si - v_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = FastCausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xylose': np.float64(0.11866853374454497), 'xanthosine': np.float64(-0.018385377073444314), 'uracil': np.float64(0.09265538907924285), 'ribulose/xylulose': np.float64(-0.01608475582038705), 'valylglutamine': np.float64(0.0015482242274281917), 'tryptophylglycine': np.float64(-0.006556190949174268), 'succinate': np.float64(0.09336936802891681), 'valine betaine': np.float64(-0.031943003398665654), 'ursodeoxycholate sulfate (1)': np.float64(0.002086056952346538), 'tricarballylate': np.float64(0.002178932709545898), 'succinimide': np.float64(-0.015894412241578462), 'thymine': np.float64(0.03755382803723222), 'syringic acid': np.float64(0.0033322343721636136), 'serotonin': np.float64(-0.07955677350938707), 'ribitol': np.float64(-0.019355355098207676)}\n"
     ]
    }
   ],
   "source": [
    "phi = ci.compute_modified_shap_proba(x_instance, is_classifier=True)\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = []\n",
    "for i in range(len(X_test)):\n",
    "    phi.append(ci.compute_modified_shap_proba(X_test.iloc[i], is_classifier=True))\n",
    "\n",
    "phi_df = pd.DataFrame(phi)\n",
    "mean_values = phi_df.abs().mean()\n",
    "global_importance = mean_values.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate_global_shap_scores\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 1010]\n",
    "\n",
    "X = X[[\"xylose\", \"xanthosine\", \"uracil\", \"ribulose/xylulose\", \"valylglutamine\", \"tryptophylglycine\", \"succinate\", \"valine betaine\", \"ursodeoxycholate sulfate (1)\", \"tricarballylate\",\"succinimide\", \"thymine\", \"syringic acid\", \"serotonin\", \"ribitol\" ]]\n",
    "\n",
    "y = df_encoded['Group']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X),columns=X.columns,index=X.index)\n",
    "\n",
    "all_scores = {\n",
    "    'deletion': {\n",
    "        'auroc': [],\n",
    "        'cross_entropy': [],\n",
    "        'brier': []\n",
    "    },\n",
    "    'insertion': {\n",
    "        'auroc': [],\n",
    "        'cross_entropy': [],\n",
    "        'brier': []\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in seeds:\n",
    "    print(\"Training Random Forest model...\")\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 7],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "    estimator=rf, param_distributions=param_dist, n_iter=50,\n",
    "            cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    shap_values = global_importance\n",
    "\n",
    "    result = evaluate_global_shap_scores(model, X_test, y_test, shap_values, causal=True)\n",
    "\n",
    "    for method in ['deletion', 'insertion']:\n",
    "        for metric in ['auroc', 'cross_entropy', 'brier']:\n",
    "            all_scores[method][metric].append(result[method][\"average_scores\"][metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "{\n",
      "    \"deletion\": {\n",
      "        \"auroc\": {\n",
      "            \"mean\": 0.6072804297593022,\n",
      "            \"std\": 0.033756452674940034\n",
      "        },\n",
      "        \"cross_entropy\": {\n",
      "            \"mean\": 0.7810383556603926,\n",
      "            \"std\": 0.04125080773121551\n",
      "        },\n",
      "        \"brier\": {\n",
      "            \"mean\": 0.29054916153982324,\n",
      "            \"std\": 0.018640170603287803\n",
      "        }\n",
      "    },\n",
      "    \"insertion\": {\n",
      "        \"auroc\": {\n",
      "            \"mean\": 0.8531238679722886,\n",
      "            \"std\": 0.01749174187306101\n",
      "        },\n",
      "        \"cross_entropy\": {\n",
      "            \"mean\": 0.4902830011310785,\n",
      "            \"std\": 0.02738522128428254\n",
      "        },\n",
      "        \"brier\": {\n",
      "            \"mean\": 0.15868453686398365,\n",
      "            \"std\": 0.011631364668156507\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "final_results = {\n",
    "    'deletion': {\n",
    "        metric: {\n",
    "            'mean': np.mean(scores),\n",
    "            'std': np.std(scores)\n",
    "        }\n",
    "        for metric, scores in all_scores['deletion'].items()\n",
    "    },\n",
    "    'insertion': {\n",
    "        metric: {\n",
    "            'mean': np.mean(scores),\n",
    "            'std': np.std(scores)\n",
    "        }\n",
    "        for metric, scores in all_scores['insertion'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(json.dumps(final_results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
