{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization inspired by TreeSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from causal_inference import CausalInference\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "base_dir = '../../'\n",
    "result_dir = base_dir + 'result/R/'\n",
    "data_path = base_dir + 'dataset/' + 'Real_World_IBS.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "df = df.drop(columns=['HAD_Anxiety', 'Patient', 'Batch_metabolomics', 'BH', 'Sex', 'Age', 'BMI','Race','Education','HAD_Depression','STAI_Tanxiety', 'Diet_Category','Diet_Pattern'])\n",
    "label_encoder = LabelEncoder()\n",
    "df['Group'] = label_encoder.fit_transform(df['Group'])\n",
    "df_encoded = df\n",
    "\n",
    "X = df_encoded.drop(columns=['Group'])\n",
    "y = df_encoded['Group']\n",
    "\n",
    "X = X[[\"xylose\", \"xanthosine\", \"uracil\", \"ribulose/xylulose\", \"valylglutamine\",\n",
    "           \"tryptophylglycine\", \"succinate\", \"valine betaine\", \"ursodeoxycholate sulfate (1)\",\n",
    "           \"tricarballylate\", \"succinimide\", \"thymine\", \"syringic acid\", \"serotonin\", \"ribitol\"]]\n",
    "\n",
    "param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 7],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "estimator=rf, param_distributions=param_dist, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = CausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]\n",
    "\n",
    "phi = ci.compute_modified_shap_proba(x_instance, is_classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 61.2101 s\n",
      "File: c:\\Users\\snorl\\Desktop\\FYP\\code\\python\\causal_inference.py\n",
      "Function: sample_conditional at line 110\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   110                                               def sample_conditional(self, feature, parent_values):\n",
      "   111                                                   \"\"\"\n",
      "   112                                                   Sample a value for a feature conditioned on its parent features using precomputed regression model.\n",
      "   113                                                   \"\"\"\n",
      "   114    461200    7621266.0     16.5      1.2          effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
      "   115    168500     745582.0      4.4      0.1          if not effective_parents:\n",
      "   116                                                       return self.sample_marginal(feature)\n",
      "   117    168500    2127240.0     12.6      0.3          model_key = (feature, tuple(sorted(effective_parents))) \n",
      "   118    168500    1714998.0     10.2      0.3          if model_key not in self.regression_models:\n",
      "   119         7      67484.0   9640.6      0.0              X = self.data[effective_parents].values\n",
      "   120         7       8399.0   1199.9      0.0              y = self.data[feature].values\n",
      "   121         7        332.0     47.4      0.0              reg = LinearRegression()\n",
      "   122         7     169728.0  24246.9      0.0              reg.fit(X, y)\n",
      "   123         7      24514.0   3502.0      0.0              residuals = y - reg.predict(X)\n",
      "   124         7       5578.0    796.9      0.0              std = residuals.std()\n",
      "   125         7         73.0     10.4      0.0              self.regression_models[model_key] = (reg, std)\n",
      "   126    168500    1281531.0      7.6      0.2          reg, std = self.regression_models[model_key]\n",
      "   127    461200    8126982.0     17.6      1.3          parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
      "   128    168500  581139265.0   3448.9     94.9          mean = reg.predict(parent_values_array)[0]\n",
      "   129    168500    6862608.0     40.7      1.1          sampled_value = np.random.normal(mean, std)\n",
      "   130    168500    2205639.0     13.1      0.4          return sampled_value\n",
      "\n",
      "Total time: 167.805 s\n",
      "File: c:\\Users\\snorl\\Desktop\\FYP\\code\\python\\causal_inference.py\n",
      "Function: compute_v_do at line 132\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   132                                               def compute_v_do(self, S, x_S, num_samples=50, is_classifier=False):\n",
      "   133       770       5695.0      7.4      0.0          samples_list = []\n",
      "   134       770    6056846.0   7866.0      0.4          variables_order = self.get_topological_order(S)\n",
      "   135                                                   \n",
      "   136     39270     249626.0      6.4      0.0          for _ in range(num_samples):\n",
      "   137     38500     165015.0      4.3      0.0              sample = {}\n",
      "   138    242550    1101728.0      4.5      0.1              for feature in S:\n",
      "   139    204050   32414567.0    158.9      1.9                  sample[feature] = x_S[feature]\n",
      "   140    654500    3806337.0      5.8      0.2              for feature in variables_order:\n",
      "   141    616000    4092621.0      6.6      0.2                  if feature in S or feature == self.target_variable:\n",
      "   142    242550     794026.0      3.3      0.0                      continue\n",
      "   143    373450   16566827.0     44.4      1.0                  parents = self.get_parents(feature)\n",
      "   144    666150   22399934.0     33.6      1.3                  parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
      "   145    373450    1805338.0      4.8      0.1                  if not parent_values:\n",
      "   146    204950  682932049.0   3332.2     40.7                      sample[feature] = self.sample_marginal(feature)\n",
      "   147                                                           else:\n",
      "   148    168500  625618699.0   3712.9     37.3                      sample[feature] = self.sample_conditional(feature, parent_values)\n",
      "   149     38500     231183.0      6.0      0.0              samples_list.append(sample)\n",
      "   150                                                   \n",
      "   151       770   11580007.0  15039.0      0.7          intervened_data = pd.DataFrame(samples_list)\n",
      "   152       770   10161578.0  13196.9      0.6          intervened_data = intervened_data[self.model.feature_names_in_]\n",
      "   153       770       5587.0      7.3      0.0          if is_classifier:\n",
      "   154                                                       probas = self.model.predict_proba(intervened_data)[:, 1]\n",
      "   155                                                   else:\n",
      "   156       770  257500250.0 334415.9     15.3              probas = self.model.predict(intervened_data)\n",
      "   157       770     564160.0    732.7      0.0          return np.mean(probas)\n",
      "\n",
      "Total time: 170.541 s\n",
      "File: c:\\Users\\snorl\\Desktop\\FYP\\code\\python\\causal_inference.py\n",
      "Function: compute_modified_shap_proba at line 159\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   159                                               def compute_modified_shap_proba(self, x, num_samples=50, shap_num_samples=50, is_classifier=False):\n",
      "   160        16        367.0     22.9      0.0          features = [col for col in self.data.columns if col != self.target_variable]\n",
      "   161         1         12.0     12.0      0.0          n_features = len(features)\n",
      "   162        16         76.0      4.8      0.0          phi_causal = {feature: 0.0 for feature in features}\n",
      "   163                                           \n",
      "   164         1      14247.0  14247.0      0.0          data_without_target = self.data.drop(columns=[self.target_variable], errors='ignore')\n",
      "   165         1       9964.0   9964.0      0.0          data_without_target = data_without_target[self.model.feature_names_in_]\n",
      "   166         1          5.0      5.0      0.0          if is_classifier:\n",
      "   167                                                       E_fX = self.model.predict_proba(data_without_target)[:, 1].mean() \n",
      "   168                                                   else:\n",
      "   169         1     486715.0 486715.0      0.0              E_fX = self.model.predict(data_without_target).mean()\n",
      "   170                                           \n",
      "   171         1      12222.0  12222.0      0.0          x_ordered = x[self.model.feature_names_in_]\n",
      "   172         1         12.0     12.0      0.0          if is_classifier:\n",
      "   173                                                       f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]  \n",
      "   174                                                   else:\n",
      "   175         1     353599.0 353599.0      0.0              f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
      "   176                                           \n",
      "   177        51        418.0      8.2      0.0          for _ in range(shap_num_samples):\n",
      "   178        50       9645.0    192.9      0.0              S_size = random.randint(0, n_features)\n",
      "   179        50      22996.0    459.9      0.0              S = random.sample(features, S_size)\n",
      "   180       800       7302.0      9.1      0.0              for i in features:\n",
      "   181       750       6827.0      9.1      0.0                  if i in S:\n",
      "   182       365       1142.0      3.1      0.0                      continue\n",
      "   183       385       7742.0     20.1      0.0                  S_without_i = S.copy()\n",
      "   184       385       4724.0     12.3      0.0                  S_with_i = S + [i]\n",
      "   185       385    4601478.0  11951.9      0.3                  x_S = x[S_without_i] if S_without_i else pd.Series(dtype=float)\n",
      "   186       385    3823841.0   9932.1      0.2                  x_Si = x[S_with_i] if S_with_i else pd.Series(dtype=float)\n",
      "   187       385  875760055.0    2e+06     51.4                  v_S = self.compute_v_do(S_without_i, x_S, num_samples=num_samples, is_classifier=is_classifier)\n",
      "   188       385  820247045.0    2e+06     48.1                  v_Si = self.compute_v_do(S_with_i, x_Si, num_samples=num_samples, is_classifier=is_classifier)\n",
      "   189       385      18766.0     48.7      0.0                  weight = (factorial(len(S_without_i)) * factorial(n_features - len(S_without_i) - 1)) / factorial(n_features)\n",
      "   190       385       5776.0     15.0      0.0                  gamma_i = self.gamma.get(i, 0.0)\n",
      "   191       385       2012.0      5.2      0.0                  weight *= gamma_i\n",
      "   192       385       6836.0     17.8      0.0                  delta_v = v_Si - v_S\n",
      "   193       385      10055.0     26.1      0.0                  phi_causal[i] += weight * delta_v\n",
      "   194                                           \n",
      "   195         1         65.0     65.0      0.0          sum_phi_causal = sum(phi_causal.values())\n",
      "   196         1         13.0     13.0      0.0          if sum_phi_causal == 0:\n",
      "   197                                                       phi_normalized = {k: 0.0 for k in phi_causal.keys()}\n",
      "   198                                                   else:\n",
      "   199         1         19.0     19.0      0.0              scaling_factor = (f_x - E_fX) / sum_phi_causal\n",
      "   200        16        134.0      8.4      0.0              phi_normalized = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
      "   201                                           \n",
      "   202         1         26.0     26.0      0.0          return phi_normalized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original Causal SHAP\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "ci = CausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(ci.compute_modified_shap_proba)\n",
    "profiler.add_function(ci.compute_v_do)\n",
    "profiler.add_function(ci.sample_conditional)\n",
    "\n",
    "profiler.run('phi_normalized = ci.compute_modified_shap_proba(x_instance)')\n",
    "profiler.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with Causal Tree Path (Reduced Feature Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from causallearn.utils.cit import fisherz\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from math import factorial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n",
    "class FastCausalInference:\n",
    "    def __init__(self, data, model, target_variable):\n",
    "        self.data = data  \n",
    "        self.pc_graph = None\n",
    "        self.model = model  \n",
    "        self.gamma = None  \n",
    "        self.target_variable = target_variable \n",
    "        self.ida_graph = None\n",
    "        self.regression_models = {}\n",
    "        self.feature_depths = {}  # Store feature depths for optimization\n",
    "\n",
    "    def run_pc_algorithm(self, alpha=0.05):\n",
    "        data_np = self.data.to_numpy()\n",
    "        pc_result = pc(data_np, alpha, fisherz)\n",
    "        self.pc_graph = pc_result.G\n",
    "        return self.pc_graph\n",
    "\n",
    "    def draw_graph(self, file_path):\n",
    "        pyd = GraphUtils.to_pydot(self.pc_graph)\n",
    "        pyd.write_png(file_path)\n",
    "\n",
    "    def load_causal_strengths(self, json_file_path):\n",
    "        \"\"\"Load causal strengths (beta_i) from JSON file and compute gamma_i.\"\"\"\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            causal_effects_list = json.load(f)\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        nodes = list(self.data.columns)\n",
    "        G.add_nodes_from(nodes)\n",
    "\n",
    "        for item in causal_effects_list:\n",
    "            pair = item['Pair']\n",
    "            mean_causal_effect = item['Mean_Causal_Effect']\n",
    "            if mean_causal_effect is None:\n",
    "                continue  \n",
    "            source, target = pair.split('->')\n",
    "            source = source.strip()\n",
    "            target = target.strip()\n",
    "            G.add_edge(source, target, weight=mean_causal_effect)\n",
    "        \n",
    "        self.ida_graph = G.copy()\n",
    "        \n",
    "        # Compute feature depths for optimization\n",
    "        self._compute_feature_depths()\n",
    "        \n",
    "        features = self.data.columns.tolist()\n",
    "        beta_dict = {}\n",
    "\n",
    "        for feature in features:\n",
    "            if feature == self.target_variable:\n",
    "                continue\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(G, source=feature, target=self.target_variable))\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue  \n",
    "            total_effect = 0\n",
    "            for path in paths:\n",
    "                effect = 1\n",
    "                for i in range(len(path)-1):\n",
    "                    edge_weight = G[path[i]][path[i+1]]['weight']\n",
    "                    effect *= edge_weight\n",
    "                total_effect += effect\n",
    "            if total_effect != 0:\n",
    "                beta_dict[feature] = total_effect\n",
    "\n",
    "        total_causal_effect = sum(abs(beta) for beta in beta_dict.values())\n",
    "        if total_causal_effect == 0:\n",
    "            self.gamma = {k: 0.0 for k in features}\n",
    "        else:\n",
    "            self.gamma = {k: abs(beta_dict.get(k, 0.0)) / total_causal_effect for k in features}\n",
    "        return self.gamma\n",
    "    \n",
    "    def _compute_feature_depths(self):\n",
    "        \"\"\"Compute minimum depth of each feature to target in causal graph.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        for feature in features:\n",
    "            try:\n",
    "                all_paths = list(nx.all_simple_paths(self.ida_graph, feature, self.target_variable))\n",
    "                min_depth = float('inf')\n",
    "                for path in all_paths:\n",
    "                    depth = len(path) - 1  # Exclude target\n",
    "                    min_depth = min(min_depth, depth)\n",
    "                if min_depth != float('inf'):\n",
    "                    self.feature_depths[feature] = min_depth\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    def get_topological_order(self, S):\n",
    "        \"\"\"Returns the topological order of variables after intervening on subset S.\"\"\"\n",
    "        G_intervened = self.ida_graph.copy()\n",
    "        for feature in S:\n",
    "            G_intervened.remove_edges_from(list(G_intervened.in_edges(feature)))\n",
    "        missing_nodes = set(self.data.columns) - set(G_intervened.nodes)\n",
    "        G_intervened.add_nodes_from(missing_nodes)\n",
    "\n",
    "        try:\n",
    "            order = list(nx.topological_sort(G_intervened))\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            raise ValueError(\"The causal graph contains cycles.\")\n",
    "        \n",
    "        return order\n",
    "    \n",
    "    def get_parents(self, feature):\n",
    "        \"\"\"Returns the list of parent features for a given feature in the causal graph.\"\"\"\n",
    "        return list(self.ida_graph.predecessors(feature))\n",
    "\n",
    "    def sample_marginal(self, feature):\n",
    "        \"\"\"Sample a value from the marginal distribution of the specified feature.\"\"\"\n",
    "        return self.data[feature].sample(1).iloc[0]\n",
    "\n",
    "    def sample_conditional(self, feature, parent_values):\n",
    "        \"\"\"Sample a value for a feature conditioned on its parent features.\"\"\"\n",
    "        effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
    "        if not effective_parents:\n",
    "            return self.sample_marginal(feature)\n",
    "        model_key = (feature, tuple(sorted(effective_parents))) \n",
    "        if model_key not in self.regression_models:\n",
    "            X = self.data[effective_parents].values\n",
    "            y = self.data[feature].values\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "            residuals = y - reg.predict(X)\n",
    "            std = residuals.std()\n",
    "            self.regression_models[model_key] = (reg, std)\n",
    "        reg, std = self.regression_models[model_key]\n",
    "        parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
    "        mean = reg.predict(parent_values_array)[0]\n",
    "        sampled_value = np.random.normal(mean, std)\n",
    "        return sampled_value\n",
    "\n",
    "    def compute_v_do(self, S, x_S, num_samples=50, is_classifier=False):\n",
    "        \"\"\"Compute interventional expectations.\"\"\"\n",
    "        samples_list = []\n",
    "        variables_order = self.get_topological_order(S)\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            sample = {}\n",
    "            for feature in S:\n",
    "                sample[feature] = x_S[feature]\n",
    "            for feature in variables_order:\n",
    "                if feature in S or feature == self.target_variable:\n",
    "                    continue\n",
    "                parents = self.get_parents(feature)\n",
    "                parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
    "                if not parent_values:\n",
    "                    sample[feature] = self.sample_marginal(feature)\n",
    "                else:\n",
    "                    sample[feature] = self.sample_conditional(feature, parent_values)\n",
    "            samples_list.append(sample)\n",
    "        \n",
    "        intervened_data = pd.DataFrame(samples_list)\n",
    "        intervened_data = intervened_data[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            probas = self.model.predict_proba(intervened_data)[:, 1]\n",
    "        else:\n",
    "            probas = self.model.predict(intervened_data)\n",
    "        return np.mean(probas)\n",
    "\n",
    "    def compute_modified_shap_proba(self, x, num_samples=50, shap_num_samples=50, is_classifier=False):\n",
    "        \"\"\"Compute modified SHAP values using depth-based optimization.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        n_features = len(features)\n",
    "        phi_causal = {feature: 0.0 for feature in features}\n",
    "\n",
    "        data_without_target = self.data.drop(columns=[self.target_variable], errors='ignore')\n",
    "        data_without_target = data_without_target[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            E_fX = self.model.predict_proba(data_without_target)[:, 1].mean() \n",
    "        else:\n",
    "            E_fX = self.model.predict(data_without_target).mean()\n",
    "\n",
    "        x_ordered = x[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]  \n",
    "        else:\n",
    "            f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
    "\n",
    "        features_by_depth = defaultdict(list)\n",
    "        for feature in features:\n",
    "            depth = self.feature_depths.get(feature, float('inf'))\n",
    "            features_by_depth[depth].append(feature)\n",
    "\n",
    "        for depth in sorted(features_by_depth.keys()):\n",
    "            depth_features = features_by_depth[depth]\n",
    "            \n",
    "            for feature in depth_features:\n",
    "                valid_features = set()\n",
    "                for d in range(depth + 1):\n",
    "                    valid_features.update(features_by_depth[d])\n",
    "                valid_features.discard(feature)\n",
    "                \n",
    "        for _ in range(shap_num_samples):\n",
    "            S_size = random.randint(0, len(valid_features))\n",
    "            S = random.sample(list(valid_features), S_size)\n",
    "            for i in valid_features:\n",
    "                if i in S:\n",
    "                    continue \n",
    "                S_without_i = S.copy()\n",
    "                S_with_i = S + [i]\n",
    "                x_S = x[S_without_i] if S_without_i else pd.Series(dtype=float)\n",
    "                x_Si = x[S_with_i] if S_with_i else pd.Series(dtype=float)\n",
    "                v_S = self.compute_v_do(S_without_i, x_S, num_samples=num_samples, is_classifier=is_classifier)\n",
    "                v_Si = self.compute_v_do(S_with_i, x_Si, num_samples=num_samples, is_classifier=is_classifier)\n",
    "                weight = (factorial(len(S_without_i)) * factorial(n_features - len(S_without_i) - 1)) / factorial(n_features)\n",
    "                gamma_i = self.gamma.get(i, 0.0)\n",
    "                weight *= gamma_i\n",
    "                delta_v = v_Si - v_S\n",
    "                phi_causal[i] += weight * delta_v\n",
    "\n",
    "        sum_phi_causal = sum(phi_causal.values())\n",
    "        if sum_phi_causal == 0:\n",
    "            phi_normalized = {k: 0.0 for k in phi_causal.keys()}\n",
    "        else:\n",
    "            scaling_factor = (f_x - E_fX) / sum_phi_causal\n",
    "            phi_normalized = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
    "\n",
    "        return phi_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = FastCausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]\n",
    "\n",
    "phi = ci.compute_modified_shap_proba(x_instance, is_classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 63.6335 s\n",
      "File: C:\\Users\\snorl\\AppData\\Local\\Temp\\ipykernel_1332\\343667890.py\n",
      "Function: sample_conditional at line 123\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   123                                               def sample_conditional(self, feature, parent_values):\n",
      "   124                                                   \"\"\"Sample a value for a feature conditioned on its parent features.\"\"\"\n",
      "   125    446550    7975652.0     17.9      1.3          effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
      "   126    162800     782189.0      4.8      0.1          if not effective_parents:\n",
      "   127                                                       return self.sample_marginal(feature)\n",
      "   128    162800    2283014.0     14.0      0.4          model_key = (feature, tuple(sorted(effective_parents))) \n",
      "   129    162800    1948488.0     12.0      0.3          if model_key not in self.regression_models:\n",
      "   130         7      61097.0   8728.1      0.0              X = self.data[effective_parents].values\n",
      "   131         7       2613.0    373.3      0.0              y = self.data[feature].values\n",
      "   132         7        348.0     49.7      0.0              reg = LinearRegression()\n",
      "   133         7     151898.0  21699.7      0.0              reg.fit(X, y)\n",
      "   134         7      23044.0   3292.0      0.0              residuals = y - reg.predict(X)\n",
      "   135         7       5510.0    787.1      0.0              std = residuals.std()\n",
      "   136         7         94.0     13.4      0.0              self.regression_models[model_key] = (reg, std)\n",
      "   137    162800    1328619.0      8.2      0.2          reg, std = self.regression_models[model_key]\n",
      "   138    446550    8660688.0     19.4      1.4          parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
      "   139    162800  603374061.0   3706.2     94.8          mean = reg.predict(parent_values_array)[0]\n",
      "   140    162800    7429776.0     45.6      1.2          sampled_value = np.random.normal(mean, std)\n",
      "   141    162800    2308186.0     14.2      0.4          return sampled_value\n",
      "\n",
      "Total time: 168.779 s\n",
      "File: C:\\Users\\snorl\\AppData\\Local\\Temp\\ipykernel_1332\\343667890.py\n",
      "Function: compute_v_do at line 143\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   143                                               def compute_v_do(self, S, x_S, num_samples=50, is_classifier=False):\n",
      "   144                                                   \"\"\"Compute interventional expectations.\"\"\"\n",
      "   145       698       5182.0      7.4      0.0          samples_list = []\n",
      "   146       698    5772507.0   8270.1      0.3          variables_order = self.get_topological_order(S)\n",
      "   147                                                   \n",
      "   148     35598     263986.0      7.4      0.0          for _ in range(num_samples):\n",
      "   149     34900     173556.0      5.0      0.0              sample = {}\n",
      "   150    209450    1040486.0      5.0      0.1              for feature in S:\n",
      "   151    174550   29903127.0    171.3      1.8                  sample[feature] = x_S[feature]\n",
      "   152    593300    3805555.0      6.4      0.2              for feature in variables_order:\n",
      "   153    558400    4027124.0      7.2      0.2                  if feature in S or feature == self.target_variable:\n",
      "   154    209450     728606.0      3.5      0.0                      continue\n",
      "   155    348950   16680627.0     47.8      1.0                  parents = self.get_parents(feature)\n",
      "   156    632700   23252155.0     36.8      1.4                  parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
      "   157    348950    1792776.0      5.1      0.1                  if not parent_values:\n",
      "   158    186150  674790178.0   3625.0     40.0                      sample[feature] = self.sample_marginal(feature)\n",
      "   159                                                           else:\n",
      "   160    162800  650353153.0   3994.8     38.5                      sample[feature] = self.sample_conditional(feature, parent_values)\n",
      "   161     34900     260880.0      7.5      0.0              samples_list.append(sample)\n",
      "   162                                                   \n",
      "   163       698   11386390.0  16312.9      0.7          intervened_data = pd.DataFrame(samples_list)\n",
      "   164       698   10086857.0  14451.1      0.6          intervened_data = intervened_data[self.model.feature_names_in_]\n",
      "   165       698       5441.0      7.8      0.0          if is_classifier:\n",
      "   166       698  252942161.0 362381.3     15.0              probas = self.model.predict_proba(intervened_data)[:, 1]\n",
      "   167                                                   else:\n",
      "   168                                                       probas = self.model.predict(intervened_data)\n",
      "   169       698     524139.0    750.9      0.0          return np.mean(probas)\n",
      "\n",
      "Total time: 171.47 s\n",
      "File: C:\\Users\\snorl\\AppData\\Local\\Temp\\ipykernel_1332\\343667890.py\n",
      "Function: compute_modified_shap_proba at line 171\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   171                                               def compute_modified_shap_proba(self, x, num_samples=50, shap_num_samples=50, is_classifier=False):\n",
      "   172                                                   \"\"\"Compute modified SHAP values using depth-based optimization.\"\"\"\n",
      "   173        16        264.0     16.5      0.0          features = [col for col in self.data.columns if col != self.target_variable]\n",
      "   174         1          7.0      7.0      0.0          n_features = len(features)\n",
      "   175        16         66.0      4.1      0.0          phi_causal = {feature: 0.0 for feature in features}\n",
      "   176                                           \n",
      "   177         1      17536.0  17536.0      0.0          data_without_target = self.data.drop(columns=[self.target_variable], errors='ignore')\n",
      "   178         1      15351.0  15351.0      0.0          data_without_target = data_without_target[self.model.feature_names_in_]\n",
      "   179         1         11.0     11.0      0.0          if is_classifier:\n",
      "   180         1     330308.0 330308.0      0.0              E_fX = self.model.predict_proba(data_without_target)[:, 1].mean() \n",
      "   181                                                   else:\n",
      "   182                                                       E_fX = self.model.predict(data_without_target).mean()\n",
      "   183                                           \n",
      "   184         1      10036.0  10036.0      0.0          x_ordered = x[self.model.feature_names_in_]\n",
      "   185         1          6.0      6.0      0.0          if is_classifier:\n",
      "   186         1     265753.0 265753.0      0.0              f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]  \n",
      "   187                                                   else:\n",
      "   188                                                       f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
      "   189                                           \n",
      "   190         1         26.0     26.0      0.0          features_by_depth = defaultdict(list)\n",
      "   191        16         84.0      5.2      0.0          for feature in features:\n",
      "   192        15        127.0      8.5      0.0              depth = self.feature_depths.get(feature, float('inf'))\n",
      "   193        15         91.0      6.1      0.0              features_by_depth[depth].append(feature)\n",
      "   194                                           \n",
      "   195         6         62.0     10.3      0.0          for depth in sorted(features_by_depth.keys()):\n",
      "   196         5         23.0      4.6      0.0              depth_features = features_by_depth[depth]\n",
      "   197                                                       \n",
      "   198        20         73.0      3.6      0.0              for feature in depth_features:\n",
      "   199        15         79.0      5.3      0.0                  valid_features = set()\n",
      "   200        81        320.0      4.0      0.0                  for d in range(depth + 1):\n",
      "   201        66        310.0      4.7      0.0                      valid_features.update(features_by_depth[d])\n",
      "   202        15         56.0      3.7      0.0                  valid_features.discard(feature)\n",
      "   203                                                           \n",
      "   204        51        454.0      8.9      0.0          for _ in range(shap_num_samples):\n",
      "   205        50      18790.0    375.8      0.0              S_size = random.randint(0, len(valid_features))\n",
      "   206        50      25083.0    501.7      0.0              S = random.sample(list(valid_features), S_size)\n",
      "   207       750       8398.0     11.2      0.0              for i in valid_features:\n",
      "   208       700       6423.0      9.2      0.0                  if i in S:\n",
      "   209       351       1219.0      3.5      0.0                      continue \n",
      "   210       349       5657.0     16.2      0.0                  S_without_i = S.copy()\n",
      "   211       349       4882.0     14.0      0.0                  S_with_i = S + [i]\n",
      "   212       349    4459976.0  12779.3      0.3                  x_S = x[S_without_i] if S_without_i else pd.Series(dtype=float)\n",
      "   213       349    3583897.0  10269.0      0.2                  x_Si = x[S_with_i] if S_with_i else pd.Series(dtype=float)\n",
      "   214       349  888631847.0    3e+06     51.8                  v_S = self.compute_v_do(S_without_i, x_S, num_samples=num_samples, is_classifier=is_classifier)\n",
      "   215       349  817272759.0    2e+06     47.7                  v_Si = self.compute_v_do(S_with_i, x_Si, num_samples=num_samples, is_classifier=is_classifier)\n",
      "   216       349      18206.0     52.2      0.0                  weight = (factorial(len(S_without_i)) * factorial(n_features - len(S_without_i) - 1)) / factorial(n_features)\n",
      "   217       349       5271.0     15.1      0.0                  gamma_i = self.gamma.get(i, 0.0)\n",
      "   218       349       1903.0      5.5      0.0                  weight *= gamma_i\n",
      "   219       349       5673.0     16.3      0.0                  delta_v = v_Si - v_S\n",
      "   220       349       9913.0     28.4      0.0                  phi_causal[i] += weight * delta_v\n",
      "   221                                           \n",
      "   222         1         60.0     60.0      0.0          sum_phi_causal = sum(phi_causal.values())\n",
      "   223         1         11.0     11.0      0.0          if sum_phi_causal == 0:\n",
      "   224                                                       phi_normalized = {k: 0.0 for k in phi_causal.keys()}\n",
      "   225                                                   else:\n",
      "   226         1          8.0      8.0      0.0              scaling_factor = (f_x - E_fX) / sum_phi_causal\n",
      "   227        16        117.0      7.3      0.0              phi_normalized = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
      "   228                                           \n",
      "   229         1         34.0     34.0      0.0          return phi_normalized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimized Causal SHAP\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "ci = FastCausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(ci.compute_modified_shap_proba)\n",
    "profiler.add_function(ci.compute_v_do)\n",
    "profiler.add_function(ci.sample_conditional)\n",
    "\n",
    "profiler.run('phi = ci.compute_modified_shap_proba(x_instance, is_classifier=True)')\n",
    "profiler.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Path Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from causallearn.utils.cit import fisherz\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from math import factorial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n",
    "class OptimizedCausalInference:\n",
    "    def __init__(self, data, model, target_variable):\n",
    "        self.data = data  \n",
    "        self.pc_graph = None\n",
    "        self.model = model  \n",
    "        self.gamma = None  \n",
    "        self.target_variable = target_variable \n",
    "        self.ida_graph = None\n",
    "        self.regression_models = {}\n",
    "        self.feature_depths = {}  \n",
    "        self.path_cache = {}\n",
    "\n",
    "    def run_pc_algorithm(self, alpha=0.05):\n",
    "        data_np = self.data.to_numpy()\n",
    "        pc_result = pc(data_np, alpha, fisherz)\n",
    "        self.pc_graph = pc_result.G\n",
    "        return self.pc_graph\n",
    "\n",
    "    def draw_graph(self, file_path):\n",
    "        pyd = GraphUtils.to_pydot(self.pc_graph)\n",
    "        pyd.write_png(file_path)\n",
    "\n",
    "    def load_causal_strengths(self, json_file_path):\n",
    "        \"\"\"Load causal strengths (beta_i) from JSON file and compute gamma_i.\"\"\"\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            causal_effects_list = json.load(f)\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        nodes = list(self.data.columns)\n",
    "        G.add_nodes_from(nodes)\n",
    "\n",
    "        for item in causal_effects_list:\n",
    "            pair = item['Pair']\n",
    "            mean_causal_effect = item['Mean_Causal_Effect']\n",
    "            if mean_causal_effect is None:\n",
    "                continue  \n",
    "            source, target = pair.split('->')\n",
    "            source = source.strip()\n",
    "            target = target.strip()\n",
    "            G.add_edge(source, target, weight=mean_causal_effect)\n",
    "        \n",
    "        self.ida_graph = G.copy()\n",
    "        self._compute_feature_depths()\n",
    "        \n",
    "        features = self.data.columns.tolist()\n",
    "        beta_dict = {}\n",
    "\n",
    "        for feature in features:\n",
    "            if feature == self.target_variable:\n",
    "                continue\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(G, source=feature, target=self.target_variable))\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue  \n",
    "            total_effect = 0\n",
    "            for path in paths:\n",
    "                effect = 1\n",
    "                for i in range(len(path)-1):\n",
    "                    edge_weight = G[path[i]][path[i+1]]['weight']\n",
    "                    effect *= edge_weight\n",
    "                total_effect += effect\n",
    "            if total_effect != 0:\n",
    "                beta_dict[feature] = total_effect\n",
    "\n",
    "        total_causal_effect = sum(abs(beta) for beta in beta_dict.values())\n",
    "        if total_causal_effect == 0:\n",
    "            self.gamma = {k: 0.0 for k in features}\n",
    "        else:\n",
    "            self.gamma = {k: abs(beta_dict.get(k, 0.0)) / total_causal_effect for k in features}\n",
    "        return self.gamma\n",
    "    \n",
    "    def _compute_feature_depths(self):\n",
    "        \"\"\"Compute minimum depth of each feature to target in causal graph.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        for feature in features:\n",
    "            try:\n",
    "                all_paths = list(nx.all_simple_paths(self.ida_graph, feature, self.target_variable))\n",
    "                min_depth = float('inf')\n",
    "                for path in all_paths:\n",
    "                    depth = len(path) - 1  # Exclude target\n",
    "                    min_depth = min(min_depth, depth)\n",
    "                if min_depth != float('inf'):\n",
    "                    self.feature_depths[feature] = min_depth\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    def get_topological_order(self, S):\n",
    "        \"\"\"Returns the topological order of variables after intervening on subset S.\"\"\"\n",
    "        G_intervened = self.ida_graph.copy()\n",
    "        for feature in S:\n",
    "            G_intervened.remove_edges_from(list(G_intervened.in_edges(feature)))\n",
    "        missing_nodes = set(self.data.columns) - set(G_intervened.nodes)\n",
    "        G_intervened.add_nodes_from(missing_nodes)\n",
    "\n",
    "        try:\n",
    "            order = list(nx.topological_sort(G_intervened))\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            raise ValueError(\"The causal graph contains cycles.\")\n",
    "        \n",
    "        return order\n",
    "    \n",
    "    def get_parents(self, feature):\n",
    "        \"\"\"Returns the list of parent features for a given feature in the causal graph.\"\"\"\n",
    "        return list(self.ida_graph.predecessors(feature))\n",
    "\n",
    "    def sample_marginal(self, feature):\n",
    "        \"\"\"Sample a value from the marginal distribution of the specified feature.\"\"\"\n",
    "        return self.data[feature].sample(1).iloc[0]\n",
    "\n",
    "    def sample_conditional(self, feature, parent_values):\n",
    "        \"\"\"Sample a value for a feature conditioned on its parent features.\"\"\"\n",
    "        effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
    "        if not effective_parents:\n",
    "            return self.sample_marginal(feature)\n",
    "        model_key = (feature, tuple(sorted(effective_parents))) \n",
    "        if model_key not in self.regression_models:\n",
    "            X = self.data[effective_parents].values\n",
    "            y = self.data[feature].values\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "            residuals = y - reg.predict(X)\n",
    "            std = residuals.std()\n",
    "            self.regression_models[model_key] = (reg, std)\n",
    "        reg, std = self.regression_models[model_key]\n",
    "        parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
    "        mean = reg.predict(parent_values_array)[0]\n",
    "        sampled_value = np.random.normal(mean, std)\n",
    "        return sampled_value\n",
    "\n",
    "    def compute_v_do(self, S, x_S, num_samples=50, is_classifier=False):\n",
    "        \"\"\"Compute interventional expectations with caching.\"\"\"\n",
    "        cache_key = (frozenset(S), tuple(sorted(x_S.items())) if len(x_S) > 0 else tuple())\n",
    "        \n",
    "        if cache_key in self.path_cache:\n",
    "            return self.path_cache[cache_key]\n",
    "        \n",
    "        samples_list = []\n",
    "        variables_order = self.get_topological_order(S)\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            sample = {}\n",
    "            for feature in S:\n",
    "                sample[feature] = x_S[feature]\n",
    "            for feature in variables_order:\n",
    "                if feature in S or feature == self.target_variable:\n",
    "                    continue\n",
    "                parents = self.get_parents(feature)\n",
    "                parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
    "                if not parent_values:\n",
    "                    sample[feature] = self.sample_marginal(feature)\n",
    "                else:\n",
    "                    sample[feature] = self.sample_conditional(feature, parent_values)\n",
    "            samples_list.append(sample)\n",
    "        \n",
    "        intervened_data = pd.DataFrame(samples_list)\n",
    "        intervened_data = intervened_data[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            probas = self.model.predict_proba(intervened_data)[:, 1]\n",
    "        else:\n",
    "            probas = self.model.predict(intervened_data)\n",
    "        \n",
    "        result = np.mean(probas)\n",
    "        self.path_cache[cache_key] = result\n",
    "        return result\n",
    "\n",
    "    def compute_modified_shap_proba(self, x, num_samples=50, shap_num_samples=50, is_classifier=False):\n",
    "        \"\"\"Compute modified SHAP values using depth-based optimization.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        n_features = len(features)\n",
    "        phi_causal = {feature: 0.0 for feature in features}\n",
    "\n",
    "        data_without_target = self.data.drop(columns=[self.target_variable], errors='ignore')\n",
    "        data_without_target = data_without_target[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            E_fX = self.model.predict_proba(data_without_target)[:, 1].mean() \n",
    "        else:\n",
    "            E_fX = self.model.predict(data_without_target).mean()\n",
    "\n",
    "        x_ordered = x[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]  \n",
    "        else:\n",
    "            f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
    "\n",
    "        features_by_depth = defaultdict(list)\n",
    "        for feature in features:\n",
    "            depth = self.feature_depths.get(feature, float('inf'))\n",
    "            features_by_depth[depth].append(feature)\n",
    "\n",
    "        for depth in sorted(features_by_depth.keys()):\n",
    "            depth_features = features_by_depth[depth]\n",
    "            \n",
    "            for feature in depth_features:\n",
    "                valid_features = set()\n",
    "                for d in range(depth + 1):\n",
    "                    valid_features.update(features_by_depth[d])\n",
    "                valid_features.discard(feature)\n",
    "                \n",
    "        for _ in range(shap_num_samples):\n",
    "            S_size = random.randint(0, len(valid_features))\n",
    "            S = random.sample(list(valid_features), S_size)\n",
    "            for i in valid_features:\n",
    "                if i in S:\n",
    "                    continue \n",
    "                S_without_i = S.copy()\n",
    "                S_with_i = S + [i]\n",
    "                x_S = x[S_without_i] if S_without_i else pd.Series(dtype=float)\n",
    "                x_Si = x[S_with_i] if S_with_i else pd.Series(dtype=float)\n",
    "                v_S = self.compute_v_do(S_without_i, x_S, num_samples=num_samples, is_classifier=is_classifier)\n",
    "                v_Si = self.compute_v_do(S_with_i, x_Si, num_samples=num_samples, is_classifier=is_classifier)\n",
    "                weight = (factorial(len(S_without_i)) * factorial(n_features - len(S_without_i) - 1)) / factorial(n_features)\n",
    "                gamma_i = self.gamma.get(i, 0.0)\n",
    "                weight *= gamma_i\n",
    "                delta_v = v_Si - v_S\n",
    "                phi_causal[i] += weight * delta_v\n",
    "\n",
    "        sum_phi_causal = sum(phi_causal.values())\n",
    "        if sum_phi_causal == 0:\n",
    "            phi_normalized = {k: 0.0 for k in phi_causal.keys()}\n",
    "        else:\n",
    "            scaling_factor = (f_x - E_fX) / sum_phi_causal\n",
    "            phi_normalized = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
    "\n",
    "        return phi_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = OptimizedCausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]\n",
    "\n",
    "phi = ci.compute_modified_shap_proba(x_instance, is_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with TreeSHAP idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from causallearn.utils.cit import fisherz\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from math import factorial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n",
    "class TreeShapCausalInference:\n",
    "    def __init__(self, data, model, target_variable):\n",
    "        self.data = data  \n",
    "        self.model = model  \n",
    "        self.gamma = None  \n",
    "        self.target_variable = target_variable \n",
    "        self.ida_graph = None\n",
    "        self.regression_models = {}\n",
    "        self.feature_depths = {}\n",
    "        self.path_cache = {}\n",
    "        self.causal_paths = {}  \n",
    "        \n",
    "    def _compute_causal_paths(self):\n",
    "        \"\"\"Compute and store all causal paths to target for each feature.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        for feature in features:\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(self.ida_graph, feature, self.target_variable))\n",
    "                path_features = set()\n",
    "                for path in paths:\n",
    "                    path_features.update(path[:-1]) \n",
    "                self.causal_paths[feature] = path_features\n",
    "            except nx.NetworkXNoPath:\n",
    "                self.causal_paths[feature] = set()\n",
    "\n",
    "    def load_causal_strengths(self, json_file_path):\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            causal_effects_list = json.load(f)\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        nodes = list(self.data.columns)\n",
    "        G.add_nodes_from(nodes)\n",
    "\n",
    "        for item in causal_effects_list:\n",
    "            pair = item['Pair']\n",
    "            mean_causal_effect = item['Mean_Causal_Effect']\n",
    "            if mean_causal_effect is None:\n",
    "                continue  \n",
    "            source, target = pair.split('->')\n",
    "            source = source.strip()\n",
    "            target = target.strip()\n",
    "            G.add_edge(source, target, weight=mean_causal_effect)\n",
    "        self.ida_graph = G.copy()\n",
    "        self._compute_feature_depths()\n",
    "        self._compute_causal_paths()\n",
    "        features = self.data.columns.tolist()\n",
    "        beta_dict = {}\n",
    "\n",
    "        for feature in features:\n",
    "            if feature == self.target_variable:\n",
    "                continue\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(G, source=feature, target=self.target_variable))\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue  \n",
    "            total_effect = 0\n",
    "            for path in paths:\n",
    "                effect = 1\n",
    "                for i in range(len(path)-1):\n",
    "                    edge_weight = G[path[i]][path[i+1]]['weight']\n",
    "                    effect *= edge_weight\n",
    "                total_effect += effect\n",
    "            if total_effect != 0:\n",
    "                beta_dict[feature] = total_effect\n",
    "\n",
    "        total_causal_effect = sum(abs(beta) for beta in beta_dict.values())\n",
    "        if total_causal_effect == 0:\n",
    "            self.gamma = {k: 0.0 for k in features}\n",
    "        else:\n",
    "            self.gamma = {k: abs(beta_dict.get(k, 0.0)) / total_causal_effect for k in features}\n",
    "        return self.gamma\n",
    "    \n",
    "    def _compute_feature_depths(self):\n",
    "        \"\"\"Compute minimum depth of each feature to target in causal graph.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        for feature in features:\n",
    "            try:\n",
    "                all_paths = list(nx.all_simple_paths(self.ida_graph, feature, self.target_variable))\n",
    "                min_depth = float('inf')\n",
    "                for path in all_paths:\n",
    "                    depth = len(path) - 1  \n",
    "                    min_depth = min(min_depth, depth)\n",
    "                if min_depth != float('inf'):\n",
    "                    self.feature_depths[feature] = min_depth\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    def get_topological_order(self, S):\n",
    "        \"\"\"Returns the topological order of variables after intervening on subset S.\"\"\"\n",
    "        G_intervened = self.ida_graph.copy()\n",
    "        for feature in S:\n",
    "            G_intervened.remove_edges_from(list(G_intervened.in_edges(feature)))\n",
    "        missing_nodes = set(self.data.columns) - set(G_intervened.nodes)\n",
    "        G_intervened.add_nodes_from(missing_nodes)\n",
    "\n",
    "        try:\n",
    "            order = list(nx.topological_sort(G_intervened))\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            raise ValueError(\"The causal graph contains cycles.\")\n",
    "        \n",
    "        return order\n",
    "    \n",
    "    def get_parents(self, feature):\n",
    "        \"\"\"Returns the list of parent features for a given feature in the causal graph.\"\"\"\n",
    "        return list(self.ida_graph.predecessors(feature))\n",
    "\n",
    "    def sample_marginal(self, feature):\n",
    "        \"\"\"Sample a value from the marginal distribution of the specified feature.\"\"\"\n",
    "        return self.data[feature].sample(1).iloc[0]\n",
    "\n",
    "    def sample_conditional(self, feature, parent_values):\n",
    "        \"\"\"Sample a value for a feature conditioned on its parent features.\"\"\"\n",
    "        effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
    "        if not effective_parents:\n",
    "            return self.sample_marginal(feature)\n",
    "        model_key = (feature, tuple(sorted(effective_parents))) \n",
    "        if model_key not in self.regression_models:\n",
    "            X = self.data[effective_parents].values\n",
    "            y = self.data[feature].values\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "            residuals = y - reg.predict(X)\n",
    "            std = residuals.std()\n",
    "            self.regression_models[model_key] = (reg, std)\n",
    "        reg, std = self.regression_models[model_key]\n",
    "        parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
    "        mean = reg.predict(parent_values_array)[0]\n",
    "        sampled_value = np.random.normal(mean, std)\n",
    "        return sampled_value\n",
    "\n",
    "    def compute_v_do(self, S, x_S, num_samples=50, is_classifier=False):\n",
    "        \"\"\"Compute interventional expectations with caching.\"\"\"\n",
    "        cache_key = (frozenset(S), tuple(sorted(x_S.items())) if len(x_S) > 0 else tuple())\n",
    "        \n",
    "        if cache_key in self.path_cache:\n",
    "            return self.path_cache[cache_key]\n",
    "        \n",
    "        samples_list = []\n",
    "        variables_order = self.get_topological_order(S)\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            sample = {}\n",
    "            for feature in S:\n",
    "                sample[feature] = x_S[feature]\n",
    "            for feature in variables_order:\n",
    "                if feature in S or feature == self.target_variable:\n",
    "                    continue\n",
    "                parents = self.get_parents(feature)\n",
    "                parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
    "                if not parent_values:\n",
    "                    sample[feature] = self.sample_marginal(feature)\n",
    "                else:\n",
    "                    sample[feature] = self.sample_conditional(feature, parent_values)\n",
    "            samples_list.append(sample)\n",
    "        \n",
    "        intervened_data = pd.DataFrame(samples_list)\n",
    "        intervened_data = intervened_data[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            probas = self.model.predict_proba(intervened_data)[:, 1]\n",
    "        else:\n",
    "            probas = self.model.predict(intervened_data)\n",
    "        \n",
    "        result = np.mean(probas)\n",
    "        self.path_cache[cache_key] = result\n",
    "        return result\n",
    "\n",
    "    def is_on_causal_path(self, feature, S, target_feature):\n",
    "        \"\"\"Check if feature is on any causal path from S to target_feature.\"\"\"\n",
    "        if target_feature not in self.causal_paths:\n",
    "            return False\n",
    "        path_features = self.causal_paths[target_feature]\n",
    "        return feature in path_features\n",
    "\n",
    "    def compute_modified_shap_proba(self, x, num_samples=50, shap_num_samples=50, is_classifier=False):\n",
    "        \"\"\"TreeSHAP-inspired computation of SHAP values.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        n_features = len(features)\n",
    "        phi_causal = {feature: 0.0 for feature in features}\n",
    "\n",
    "        data_without_target = self.data.drop(columns=[self.target_variable], errors='ignore')\n",
    "        data_without_target = data_without_target[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            E_fX = self.model.predict_proba(data_without_target)[:, 1].mean() \n",
    "        else:\n",
    "            E_fX = self.model.predict(data_without_target).mean()\n",
    "\n",
    "        x_ordered = x[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]  \n",
    "        else:\n",
    "            f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
    "\n",
    "        features_by_depth = defaultdict(list)\n",
    "        for feature in features:\n",
    "            depth = self.feature_depths.get(feature, float('inf'))\n",
    "            features_by_depth[depth].append(feature)\n",
    "\n",
    "        for depth in sorted(features_by_depth.keys()):\n",
    "            depth_features = features_by_depth[depth]\n",
    "            \n",
    "            for feature in depth_features:\n",
    "                valid_features = set()\n",
    "                for d in range(depth + 1):\n",
    "                    for f in features_by_depth[d]:\n",
    "                        if f != feature and self.is_on_causal_path(f, set(), feature):\n",
    "                            valid_features.add(f)\n",
    "\n",
    "                for _ in range(shap_num_samples):\n",
    "                    S_size = random.randint(0, len(valid_features))\n",
    "                    S = random.sample(list(valid_features), S_size)\n",
    "                    relevant_features = {f for f in valid_features \n",
    "                                      if self.is_on_causal_path(f, S, feature)}\n",
    "                    \n",
    "                    if not relevant_features:\n",
    "                        continue\n",
    "                    S_without_i = S.copy()\n",
    "                    S_with_i = S + [feature]\n",
    "                    x_S = x[S_without_i] if S_without_i else pd.Series(dtype=float)\n",
    "                    x_Si = x[S_with_i] if S_with_i else pd.Series(dtype=float)\n",
    "                    \n",
    "                    v_S = self.compute_v_do(S_without_i, x_S, num_samples=num_samples, \n",
    "                                          is_classifier=is_classifier)\n",
    "                    v_Si = self.compute_v_do(S_with_i, x_Si, num_samples=num_samples, \n",
    "                                           is_classifier=is_classifier)\n",
    "                    \n",
    "                    weight = (factorial(len(relevant_features)) * \n",
    "                            factorial(n_features - len(relevant_features) - 1)) / factorial(n_features)\n",
    "                    weight *= self.gamma.get(feature, 0.0)\n",
    "                    \n",
    "                    delta_v = v_Si - v_S\n",
    "                    phi_causal[feature] += weight * delta_v\n",
    "\n",
    "        sum_phi_causal = sum(phi_causal.values())\n",
    "        if sum_phi_causal == 0:\n",
    "            phi_normalized = {k: 0.0 for k in phi_causal.keys()}\n",
    "        else:\n",
    "            scaling_factor = (f_x - E_fX) / sum_phi_causal\n",
    "            phi_normalized = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
    "\n",
    "        return phi_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = TreeShapCausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]\n",
    "\n",
    "phi = ci.compute_modified_shap_proba(x_instance, is_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Optimization with TreeSHAP idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "from math import factorial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n",
    "class FastCausalInference:\n",
    "    def __init__(self, data, model, target_variable):\n",
    "        self.data = data  \n",
    "        self.model = model  \n",
    "        self.gamma = None  \n",
    "        self.target_variable = target_variable \n",
    "        self.ida_graph = None\n",
    "        self.regression_models = {}\n",
    "        self.feature_depths = {}\n",
    "        self.path_cache = {}\n",
    "        self.causal_paths = {}  \n",
    "        \n",
    "    def _compute_causal_paths(self):\n",
    "        \"\"\"Compute and store all causal paths to target for each feature.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        for feature in features:\n",
    "            try:\n",
    "                # Store the actual paths instead of just the features\n",
    "                paths = list(nx.all_simple_paths(self.ida_graph, feature, self.target_variable))\n",
    "                self.causal_paths[feature] = paths\n",
    "            except nx.NetworkXNoPath:\n",
    "                self.causal_paths[feature] = []\n",
    "\n",
    "    def load_causal_strengths(self, json_file_path):\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            causal_effects_list = json.load(f)\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        nodes = list(self.data.columns)\n",
    "        G.add_nodes_from(nodes)\n",
    "\n",
    "        for item in causal_effects_list:\n",
    "            pair = item['Pair']\n",
    "            mean_causal_effect = item['Mean_Causal_Effect']\n",
    "            if mean_causal_effect is None:\n",
    "                continue  \n",
    "            source, target = pair.split('->')\n",
    "            source = source.strip()\n",
    "            target = target.strip()\n",
    "            G.add_edge(source, target, weight=mean_causal_effect)\n",
    "        self.ida_graph = G.copy()\n",
    "        self._compute_feature_depths()\n",
    "        self._compute_causal_paths()\n",
    "        features = self.data.columns.tolist()\n",
    "        beta_dict = {}\n",
    "\n",
    "        for feature in features:\n",
    "            if feature == self.target_variable:\n",
    "                continue\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(G, source=feature, target=self.target_variable))\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue  \n",
    "            total_effect = 0\n",
    "            for path in paths:\n",
    "                effect = 1\n",
    "                for i in range(len(path)-1):\n",
    "                    edge_weight = G[path[i]][path[i+1]]['weight']\n",
    "                    effect *= edge_weight\n",
    "                total_effect += effect\n",
    "            if total_effect != 0:\n",
    "                beta_dict[feature] = total_effect\n",
    "\n",
    "        total_causal_effect = sum(abs(beta) for beta in beta_dict.values())\n",
    "        if total_causal_effect == 0:\n",
    "            self.gamma = {k: 0.0 for k in features}\n",
    "        else:\n",
    "            self.gamma = {k: abs(beta_dict.get(k, 0.0)) / total_causal_effect for k in features}\n",
    "        return self.gamma\n",
    "    \n",
    "    def _compute_feature_depths(self):\n",
    "        \"\"\"Compute minimum depth of each feature to target in causal graph.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        for feature in features:\n",
    "            try:\n",
    "                all_paths = list(nx.all_simple_paths(self.ida_graph, feature, self.target_variable))\n",
    "                min_depth = float('inf')\n",
    "                for path in all_paths:\n",
    "                    depth = len(path) - 1  \n",
    "                    min_depth = min(min_depth, depth)\n",
    "                if min_depth != float('inf'):\n",
    "                    self.feature_depths[feature] = min_depth\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    def get_topological_order(self, S):\n",
    "        \"\"\"Returns the topological order of variables after intervening on subset S.\"\"\"\n",
    "        G_intervened = self.ida_graph.copy()\n",
    "        for feature in S:\n",
    "            G_intervened.remove_edges_from(list(G_intervened.in_edges(feature)))\n",
    "        missing_nodes = set(self.data.columns) - set(G_intervened.nodes)\n",
    "        G_intervened.add_nodes_from(missing_nodes)\n",
    "\n",
    "        try:\n",
    "            order = list(nx.topological_sort(G_intervened))\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            raise ValueError(\"The causal graph contains cycles.\")\n",
    "        \n",
    "        return order\n",
    "    \n",
    "    def get_parents(self, feature):\n",
    "        \"\"\"Returns the list of parent features for a given feature in the causal graph.\"\"\"\n",
    "        return list(self.ida_graph.predecessors(feature))\n",
    "\n",
    "    def sample_marginal(self, feature):\n",
    "        \"\"\"Sample a value from the marginal distribution of the specified feature.\"\"\"\n",
    "        return self.data[feature].sample(1).iloc[0]\n",
    "\n",
    "    def sample_conditional(self, feature, parent_values):\n",
    "        \"\"\"Sample a value for a feature conditioned on its parent features.\"\"\"\n",
    "        effective_parents = [p for p in self.get_parents(feature) if p != self.target_variable]\n",
    "        if not effective_parents:\n",
    "            return self.sample_marginal(feature)\n",
    "        model_key = (feature, tuple(sorted(effective_parents))) \n",
    "        if model_key not in self.regression_models:\n",
    "            X = self.data[effective_parents].values\n",
    "            y = self.data[feature].values\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "            residuals = y - reg.predict(X)\n",
    "            std = residuals.std()\n",
    "            self.regression_models[model_key] = (reg, std)\n",
    "        reg, std = self.regression_models[model_key]\n",
    "        parent_values_array = np.array([parent_values[parent] for parent in effective_parents]).reshape(1, -1)\n",
    "        mean = reg.predict(parent_values_array)[0]\n",
    "        sampled_value = np.random.normal(mean, std)\n",
    "        return sampled_value\n",
    "\n",
    "    def compute_v_do(self, S, x_S, is_classifier=False):\n",
    "        \"\"\"Compute interventional expectations with caching.\"\"\"\n",
    "        cache_key = (frozenset(S), tuple(sorted(x_S.items())) if len(x_S) > 0 else tuple())\n",
    "        \n",
    "        if cache_key in self.path_cache:\n",
    "            return self.path_cache[cache_key]\n",
    "        \n",
    "        variables_order = self.get_topological_order(S)\n",
    "        \n",
    "        sample = {}\n",
    "        for feature in S:\n",
    "            sample[feature] = x_S[feature]\n",
    "        for feature in variables_order:\n",
    "            if feature in S or feature == self.target_variable:\n",
    "                continue\n",
    "            parents = self.get_parents(feature)\n",
    "            parent_values = {p: x_S[p] if p in S else sample[p] for p in parents if p != self.target_variable}\n",
    "            if not parent_values:\n",
    "                sample[feature] = self.sample_marginal(feature)\n",
    "            else:\n",
    "                sample[feature] = self.sample_conditional(feature, parent_values)\n",
    "               \n",
    "        intervened_data = pd.DataFrame([sample])\n",
    "        intervened_data = intervened_data[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            probas = self.model.predict_proba(intervened_data)[:, 1]\n",
    "        else:\n",
    "            probas = self.model.predict(intervened_data)\n",
    "        \n",
    "        result = np.mean(probas)\n",
    "        self.path_cache[cache_key] = result\n",
    "        return result\n",
    "\n",
    "    def is_on_causal_path(self, feature, S, target_feature):\n",
    "        \"\"\"Check if feature is on any causal path from S to target_feature.\"\"\"\n",
    "        if target_feature not in self.causal_paths:\n",
    "            return False\n",
    "        path_features = self.causal_paths[target_feature]\n",
    "        return feature in path_features\n",
    "\n",
    "    def compute_modified_shap_proba(self, x, num_samples=50, is_classifier=False):\n",
    "        \"\"\"TreeSHAP-inspired computation using causal paths and dynamic programming.\"\"\"\n",
    "        features = [col for col in self.data.columns if col != self.target_variable]\n",
    "        n_features = len(features)\n",
    "        phi_causal = {feature: 0.0 for feature in features}\n",
    "\n",
    "        # Precompute baseline expectation\n",
    "        data_without_target = self.data.drop(columns=[self.target_variable])\n",
    "        if is_classifier:\n",
    "            E_fX = self.model.predict_proba(data_without_target)[:, 1].mean()\n",
    "        else:\n",
    "            E_fX = self.model.predict(data_without_target).mean()\n",
    "\n",
    "        # Get prediction for instance x\n",
    "        x_ordered = x[self.model.feature_names_in_]\n",
    "        if is_classifier:\n",
    "            f_x = self.model.predict_proba(x_ordered.to_frame().T)[0][1]\n",
    "        else:\n",
    "            f_x = self.model.predict(x_ordered.to_frame().T)[0]\n",
    "\n",
    "        # Process features in topological order (root to target)\n",
    "        sorted_features = sorted(features, key=lambda f: self.feature_depths.get(f, 0))\n",
    "\n",
    "        # Precompute Shapley weights for all possible subset sizes\n",
    "        max_path_length = max(self.feature_depths.values(), default=0)\n",
    "        shapley_weights = {}\n",
    "        for m in range(max_path_length + 1):\n",
    "            for d in range(m + 1, max_path_length + 1):\n",
    "                shapley_weights[(m, d)] = (factorial(m) * factorial(d - m - 1)) / factorial(d)\n",
    "\n",
    "        # Track contributions using dynamic programming (EXTEND-like logic)\n",
    "        for feature in sorted_features:\n",
    "            if feature not in self.causal_paths:\n",
    "                continue\n",
    "\n",
    "            for path in self.causal_paths[feature]:\n",
    "                path_features = [n for n in path if n != self.target_variable]\n",
    "                d = len(path_features)\n",
    "                m_values = defaultdict(float)\n",
    "\n",
    "                # Initialize with empty subset\n",
    "                m_values[0] = 1.0\n",
    "\n",
    "                # Traverse path features (EXTEND)\n",
    "                for node in path_features:\n",
    "                    if node == feature:\n",
    "                        continue  # Skip the feature itself (handled later)\n",
    "\n",
    "                    new_m_values = defaultdict(float)\n",
    "                    for m, val in m_values.items():\n",
    "                        # Include the feature\n",
    "                        new_m_values[m + 1] += val * self._get_indicator(node, x)\n",
    "                        # Exclude the feature\n",
    "                        new_m_values[m] += val * self._get_ratio(node)\n",
    "                    m_values = new_m_values\n",
    "\n",
    "                # Compute contributions for all subset sizes\n",
    "                for m in m_values:\n",
    "                    weight = shapley_weights.get((m, d), 0) * self.gamma.get(feature, 0)\n",
    "                    delta_v = self._compute_path_delta_v(feature, path, m, x, is_classifier)\n",
    "                    phi_causal[feature] += weight * delta_v * m_values[m]\n",
    "\n",
    "        # Normalize to match f(x) - E[f(X)]\n",
    "        sum_phi = sum(phi_causal.values())\n",
    "        if sum_phi != 0:\n",
    "            scaling_factor = (f_x - E_fX) / sum_phi\n",
    "            phi_causal = {k: v * scaling_factor for k, v in phi_causal.items()}\n",
    "\n",
    "        return phi_causal\n",
    "\n",
    "    def _get_indicator(self, node, x):\n",
    "        \"\"\"Check if node's value matches the intervention (simplified for illustration).\"\"\"\n",
    "        return 1.0 if node in x else 0.0  # Replace with actual causal condition check\n",
    "\n",
    "    def _get_ratio(self, node):\n",
    "        \"\"\"Get covering ratio R (from causal graph parameters).\"\"\"\n",
    "        return self.ida_graph.nodes[node].get('ratio', 1.0)  # Precompute ratios during graph setup\n",
    "        \n",
    "\n",
    "    def _compute_path_delta_v(self, feature, path, m, x, is_classifier):\n",
    "        \"\"\"Compute Δv for a causal path using precomputed expectations.\"\"\"\n",
    "        S = [n for n in path[:m] if n != feature]\n",
    "        x_S = {n: x[n] for n in S if n in x}\n",
    "        v_S = self.compute_v_do(S, x_S, is_classifier)\n",
    "\n",
    "        S_with_i = S + [feature]\n",
    "        x_Si = {**x_S, feature: x[feature]}\n",
    "        v_Si = self.compute_v_do(S_with_i, x_Si, is_classifier)\n",
    "\n",
    "        return v_Si - v_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = base_dir + 'dataset/' + 'Real_World_IBS_Predicted_Probabilities.xlsx'\n",
    "df_prob = pd.read_excel(data_path)\n",
    "X_train['Prob_Class_1'] = df_prob['Prob_Class_1']\n",
    "\n",
    "ci = FastCausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "x_instance = X_test.iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0       -0.513533        0.054241                     -0.017948   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0        -0.039505 -0.935984        1.08951  -0.124574 -0.018983   \n",
      "\n",
      "   tryptophylglycine  succinimide  xanthosine  succinate    uracil  \\\n",
      "0          -0.685612     0.094881   -0.414384  -0.500207 -0.418864   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0          -0.897152 -0.383827  \n",
      "-----------------------------------------line break----------\n",
      "     xylose  valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0 -0.651071       -0.307074       -0.279434                      0.391271   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0        -0.382151 -0.813522      -0.348835  -0.365496 -0.239429   \n",
      "\n",
      "   tryptophylglycine  succinimide  xanthosine  succinate    uracil  \\\n",
      "0          -0.664029     0.685179   -0.731309  -0.316879 -1.152711   \n",
      "\n",
      "   ribulose/xylulose  \n",
      "0          -1.457036  \n",
      "-----------------------------------------line break----------\n",
      "   valine betaine  valylglutamine  ursodeoxycholate sulfate (1)  \\\n",
      "0        -0.71469       -0.312107                     -0.510105   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0        -0.583267  1.028521      -0.223465  -0.141838 -0.880568   \n",
      "\n",
      "   tryptophylglycine  succinimide  xanthosine  succinate    uracil  \\\n",
      "0          -0.013019    -0.260238    0.666394  -0.123301 -0.022662   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0          -0.261232 -1.048527  \n",
      "-----------------------------------------line break----------\n",
      "   serotonin  valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0   0.131956        -0.63827         0.10202                      -0.46101   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid   ribitol  tryptophylglycine  \\\n",
      "0        -0.877625 -0.540344      -0.474139 -0.464427          -1.353612   \n",
      "\n",
      "   succinimide  xanthosine  succinate    uracil  ribulose/xylulose    xylose  \n",
      "0     0.536277    0.454342  -0.087409 -0.140994           0.349592  0.428873  \n",
      "-----------------------------------------line break----------\n",
      "   ribulose/xylulose  valylglutamine  valine betaine  \\\n",
      "0          -0.741389       -0.499754       -0.182894   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.249891         1.094892 -0.253438      -0.474797   \n",
      "\n",
      "   serotonin   ribitol    xylose  tryptophylglycine  succinimide  xanthosine  \\\n",
      "0  -0.165354 -0.198986  0.171556          -0.857914    -0.267568    -0.61349   \n",
      "\n",
      "   succinate    uracil  \n",
      "0  -0.048633  0.453803  \n",
      "-----------------------------------------line break----------\n",
      "   ribulose/xylulose    uracil  valylglutamine  valine betaine  \\\n",
      "0          -0.741389 -0.483208       -0.349037       -0.523229   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.510105        -0.309092 -0.512509       -0.45401   \n",
      "\n",
      "   serotonin   ribitol    xylose  tryptophylglycine  succinimide  xanthosine  \\\n",
      "0    0.10178 -0.603711 -0.276162           1.570162     3.915164   -1.371125   \n",
      "\n",
      "   succinate  \n",
      "0   1.544004  \n",
      "-----------------------------------------line break----------\n",
      "     uracil  valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0 -0.483208       -0.428569        0.280397                     -0.397318   \n",
      "\n",
      "   tricarballylate  thymine  syringic acid  serotonin   ribitol  \\\n",
      "0        -0.366293 -0.63409       1.329624  -0.023675 -0.519957   \n",
      "\n",
      "   tryptophylglycine  succinimide  xanthosine  succinate  ribulose/xylulose  \\\n",
      "0          -0.005829      2.66535     0.27054   0.788089          -1.709172   \n",
      "\n",
      "     xylose  \n",
      "0 -1.594786  \n",
      "-----------------------------------------line break----------\n",
      "   ribulose/xylulose  succinate  valylglutamine  valine betaine  \\\n",
      "0          -0.741389  -0.327249       -0.213277       -0.278172   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                      0.052986         0.116686  0.312094       2.722928   \n",
      "\n",
      "   serotonin   ribitol    xylose  tryptophylglycine  succinimide  xanthosine  \\\n",
      "0  -0.511086 -0.409504 -0.764301          -1.323791     1.596157    1.393837   \n",
      "\n",
      "     uracil  \n",
      "0 -0.267266  \n",
      "-----------------------------------------line break----------\n",
      "   succinate  valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0  -0.327249        -0.20383       -0.357603                     -0.267587   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0         1.761754 -0.273302      -0.380207   1.220146  0.226963   \n",
      "\n",
      "   tryptophylglycine  succinimide  xanthosine    uracil  ribulose/xylulose  \\\n",
      "0          -0.008116    -0.069891    0.039988 -0.753407           0.629473   \n",
      "\n",
      "     xylose  \n",
      "0  1.440892  \n",
      "-----------------------------------------line break----------\n",
      "     uracil  ribulose/xylulose  xanthosine  valylglutamine  valine betaine  \\\n",
      "0 -0.483208          -0.741389     -0.0906       -0.479854       -0.232176   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.437781         2.718026  0.411749      -0.004302   \n",
      "\n",
      "   serotonin   ribitol    xylose  tryptophylglycine  succinimide  succinate  \n",
      "0   1.568826 -0.399418 -0.350266           0.978728     -0.77594   1.117394  \n",
      "-----------------------------------------line break----------\n",
      "     uracil  xanthosine  valylglutamine  valine betaine  \\\n",
      "0 -0.483208     -0.0906          1.8652        0.252005   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                      0.147007        -0.639316  5.376559      -0.474797   \n",
      "\n",
      "   serotonin   ribitol  tryptophylglycine  succinimide  succinate  \\\n",
      "0  -0.054741  1.414434           1.252026    -0.355617   0.204856   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0           0.284167  0.961391  \n",
      "-----------------------------------------line break----------\n",
      "   xanthosine  valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0     -0.0906       -0.444812        0.339413                      -0.33244   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0         0.902585 -0.875125      -0.428278  -0.528051 -0.758026   \n",
      "\n",
      "   tryptophylglycine  succinimide   uracil  succinate  ribulose/xylulose  \\\n",
      "0          -1.539242    -1.565787 -0.12181   -0.24731          -0.814925   \n",
      "\n",
      "    xylose  \n",
      "0 -0.68839  \n",
      "-----------------------------------------line break----------\n",
      "   succinate  ribulose/xylulose  tryptophylglycine  valylglutamine  \\\n",
      "0  -0.327249          -0.741389          -0.810689       -0.516791   \n",
      "\n",
      "   valine betaine  ursodeoxycholate sulfate (1)  tricarballylate   thymine  \\\n",
      "0        1.087708                     -0.446505         0.706379 -0.990186   \n",
      "\n",
      "   syringic acid  serotonin   ribitol    xylose  succinimide  xanthosine  \\\n",
      "0        0.65674  -0.511086 -0.790677 -1.157232     1.409101    1.050621   \n",
      "\n",
      "     uracil  \n",
      "0 -0.870835  \n",
      "-----------------------------------------line break----------\n",
      "   succinate  tryptophylglycine  valylglutamine  valine betaine  \\\n",
      "0  -0.327249          -0.810689        0.240705       -0.518731   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.377124        -0.372565 -0.137512      -0.402935   \n",
      "\n",
      "   serotonin   ribitol  succinimide  xanthosine    uracil  ribulose/xylulose  \\\n",
      "0   -0.19447 -0.307938     0.944559    0.588317 -0.355174          -0.985208   \n",
      "\n",
      "     xylose  \n",
      "0 -1.585941  \n",
      "-----------------------------------------line break----------\n",
      "   tryptophylglycine  valylglutamine  valine betaine  \\\n",
      "0          -0.810689        -0.29003       -0.418204   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.065026         2.171081 -0.117105       -0.36042   \n",
      "\n",
      "   serotonin   ribitol  succinimide  xanthosine  succinate    uracil  \\\n",
      "0  -0.356655  0.440299     -0.59806    1.999665  -0.342494  0.709533   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0          -0.612733 -1.241014  \n",
      "-----------------------------------------line break----------\n",
      "   succinate  ribulose/xylulose  succinimide  valylglutamine  valine betaine  \\\n",
      "0  -0.327249          -0.741389    -0.214061         0.36693       -0.428457   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.514599        -0.583267 -0.054083       0.527085   \n",
      "\n",
      "   serotonin   ribitol    xylose  tryptophylglycine  xanthosine    uracil  \n",
      "0  -0.642592 -0.832429 -0.882387          -0.321581   -0.192952  0.167003  \n",
      "-----------------------------------------line break----------\n",
      "   succinate  succinimide  valylglutamine  valine betaine  \\\n",
      "0  -0.327249    -0.214061       -0.060421       -0.653635   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                      0.756457        -0.702998 -0.162032      -0.505413   \n",
      "\n",
      "   serotonin   ribitol  tryptophylglycine  xanthosine    uracil  \\\n",
      "0   0.386524 -0.678451          -0.555402   -1.015495 -0.743179   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0          -1.398935 -0.845542  \n",
      "-----------------------------------------line break----------\n",
      "   succinimide  valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0    -0.214061       -0.312107         0.06667                      0.331335   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0        -0.684448  0.544274      -0.455818   1.220146  0.376324   \n",
      "\n",
      "   tryptophylglycine  xanthosine  succinate    uracil  ribulose/xylulose  \\\n",
      "0          -0.139791   -0.157315   0.240741  0.955491          -1.114482   \n",
      "\n",
      "     xylose  \n",
      "0 -0.943817  \n",
      "-----------------------------------------line break----------\n",
      "     uracil  ribulose/xylulose   thymine  valylglutamine  valine betaine  \\\n",
      "0 -0.483208          -0.741389 -0.202948       -0.121641        0.054241   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate  syringic acid  serotonin  \\\n",
      "0                      -0.09358        -0.532886      -0.115245  -0.617033   \n",
      "\n",
      "    ribitol    xylose  tryptophylglycine  succinimide  xanthosine  succinate  \n",
      "0 -0.501241 -0.453814            -0.9674     0.634092    0.307035   0.071245  \n",
      "-----------------------------------------line break----------\n",
      "     uracil   thymine  valylglutamine  valine betaine  \\\n",
      "0 -0.483208 -0.202948       -0.546825        -0.53369   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate  syringic acid  serotonin  \\\n",
      "0                      0.159118          1.03471      -0.508198   -0.22977   \n",
      "\n",
      "   ribitol  tryptophylglycine  succinimide  xanthosine  succinate  \\\n",
      "0  1.52113          -0.139919    -0.779284    0.441307  -0.252603   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0          -0.864836 -0.522803  \n",
      "-----------------------------------------line break----------\n",
      "    thymine  valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0 -0.202948        2.643911       -0.184144                     -0.488328   \n",
      "\n",
      "   tricarballylate  syringic acid  serotonin   ribitol  tryptophylglycine  \\\n",
      "0        -0.955135      -0.185069   -0.62653 -0.327715           0.479411   \n",
      "\n",
      "   succinimide  xanthosine  succinate    uracil  ribulose/xylulose    xylose  \n",
      "0     0.909459    0.356402    0.41474 -0.124963          -1.263951 -1.923596  \n",
      "-----------------------------------------line break----------\n",
      "     uracil  ribulose/xylulose   ribitol  valylglutamine  valine betaine  \\\n",
      "0 -0.483208          -0.741389 -0.534382       -0.484333         1.10168   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.249891        -0.881474  2.853748      -0.143006   \n",
      "\n",
      "   serotonin    xylose  tryptophylglycine  succinimide  xanthosine  succinate  \n",
      "0   0.535276 -0.365246          -0.795822     0.119208    0.642005    0.45584  \n",
      "-----------------------------------------line break----------\n",
      "     uracil   ribitol  valylglutamine  valine betaine  \\\n",
      "0 -0.483208 -0.534382         5.94814        0.012921   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                      0.078155        -0.523333 -0.969828       7.656905   \n",
      "\n",
      "   serotonin  tryptophylglycine  succinimide  xanthosine  succinate  \\\n",
      "0  -0.365216           4.098629      2.17327    1.327922   1.475154   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0           1.965343  1.180409  \n",
      "-----------------------------------------line break----------\n",
      "    ribitol  valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0 -0.534382       -0.388315        0.674355                     -0.498357   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin  tryptophylglycine  \\\n",
      "0         0.923953  1.238445      -0.384056   0.065273            0.57231   \n",
      "\n",
      "   succinimide  xanthosine  succinate    uracil  ribulose/xylulose    xylose  \n",
      "0     0.061805    0.303978   0.533134  0.940145           0.700499  1.098052  \n",
      "-----------------------------------------line break----------\n",
      "   tryptophylglycine  succinate  ribulose/xylulose  valylglutamine  \\\n",
      "0          -0.810689  -0.327249          -0.741389       -0.382913   \n",
      "\n",
      "   valine betaine  ursodeoxycholate sulfate (1)  tricarballylate   thymine  \\\n",
      "0       -0.570829                      0.612163        -0.363587  1.033711   \n",
      "\n",
      "   syringic acid  serotonin   ribitol    xylose  succinimide  xanthosine  \\\n",
      "0      -0.366735  -0.623225  2.292865 -0.533774     0.156739   -1.619274   \n",
      "\n",
      "     uracil  \n",
      "0  0.507846  \n",
      "-----------------------------------------line break----------\n",
      "   tryptophylglycine  succinate  valylglutamine  valine betaine  \\\n",
      "0          -0.810689  -0.327249       -0.382913        0.845964   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.317641        -0.662446 -0.941435       0.010035   \n",
      "\n",
      "   serotonin   ribitol  succinimide  xanthosine    uracil  ribulose/xylulose  \\\n",
      "0  -0.134863 -0.227484     0.462018    1.004386 -0.644822           0.418256   \n",
      "\n",
      "     xylose  \n",
      "0  0.414244  \n",
      "-----------------------------------------line break----------\n",
      "   tryptophylglycine  valylglutamine  valine betaine  \\\n",
      "0          -0.810689       -0.382913       -0.613321   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  syringic acid  \\\n",
      "0                     -0.259975        -0.351579 -0.328394      -0.226953   \n",
      "\n",
      "   serotonin   ribitol  succinimide  xanthosine  succinate    uracil  \\\n",
      "0   0.701787 -0.792661     0.407023    0.176805    -0.2781  0.066625   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0           0.634898  0.556263  \n",
      "-----------------------------------------line break----------\n",
      "   valylglutamine  valine betaine  ursodeoxycholate sulfate (1)  \\\n",
      "0       -0.382913       -0.867741                     -0.480717   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0         1.094004 -1.229732      -0.089128   -0.19447  0.084908   \n",
      "\n",
      "   tryptophylglycine  succinimide  xanthosine  succinate    uracil  \\\n",
      "0           0.111766    -2.066275     0.67942   1.443776 -1.402882   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0          -0.474077 -0.750986  \n",
      "-----------------------------------------line break----------\n",
      "   tryptophylglycine  succinate  ribulose/xylulose  \\\n",
      "0          -0.810689  -0.327249          -0.741389   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  valylglutamine  valine betaine  \\\n",
      "0                     -0.164635        2.063051         1.87215   \n",
      "\n",
      "   tricarballylate  thymine  syringic acid  serotonin  ribitol    xylose  \\\n",
      "0        -0.372565  0.15918       0.121244   4.228826 -0.06018 -0.990635   \n",
      "\n",
      "   succinimide  xanthosine    uracil  \n",
      "0    -0.329739   -0.196299  0.068544  \n",
      "-----------------------------------------line break----------\n",
      "   tryptophylglycine  succinate  ursodeoxycholate sulfate (1)  valylglutamine  \\\n",
      "0          -0.810689  -0.327249                     -0.164635       -0.479854   \n",
      "\n",
      "   valine betaine  tricarballylate   thymine  syringic acid  serotonin  \\\n",
      "0       -0.549067        -0.351579 -0.990186       0.639939  -0.257183   \n",
      "\n",
      "    ribitol  succinimide  xanthosine    uracil  ribulose/xylulose    xylose  \n",
      "0  0.110701     0.805472    0.711694 -0.869043           1.247507  0.823635  \n",
      "-----------------------------------------line break----------\n",
      "   tryptophylglycine  ursodeoxycholate sulfate (1)  valylglutamine  \\\n",
      "0          -0.810689                     -0.164635         -0.1225   \n",
      "\n",
      "   valine betaine  tricarballylate   thymine  syringic acid  serotonin  \\\n",
      "0        1.274947         -0.46878 -0.365549      -0.428278  -0.414121   \n",
      "\n",
      "   ribitol  succinimide  xanthosine  succinate    uracil  ribulose/xylulose  \\\n",
      "0 -0.77543     3.019898    1.474629   0.802447 -0.305808           0.163441   \n",
      "\n",
      "     xylose  \n",
      "0 -0.240485  \n",
      "-----------------------------------------line break----------\n",
      "   ursodeoxycholate sulfate (1)  valylglutamine  valine betaine  \\\n",
      "0                     -0.164635        0.513357       -0.298836   \n",
      "\n",
      "   tricarballylate   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0        -0.057878  0.091153      -0.508198  -0.119907 -0.227484   \n",
      "\n",
      "   tryptophylglycine  succinimide  xanthosine  succinate    uracil  \\\n",
      "0          -0.269158    -0.166671    1.827497   0.934581  0.659879   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0           0.095562  0.024939  \n",
      "-----------------------------------------line break----------\n",
      "   succinimide  succinate  ribulose/xylulose  tricarballylate  valylglutamine  \\\n",
      "0    -0.214061  -0.327249          -0.741389        -0.150998         -0.5669   \n",
      "\n",
      "   valine betaine  ursodeoxycholate sulfate (1)   thymine  syringic acid  \\\n",
      "0        0.041318                      0.600719 -1.264675      -0.498231   \n",
      "\n",
      "   serotonin  ribitol    xylose  tryptophylglycine  xanthosine    uracil  \n",
      "0  -0.524928 -0.59298 -1.196197           0.732338    0.223733 -1.905862  \n",
      "-----------------------------------------line break----------\n",
      "   succinimide  succinate  tricarballylate  valylglutamine  valine betaine  \\\n",
      "0    -0.214061  -0.327249        -0.150998        0.064113       -0.450979   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0                     -0.328057  0.351168       1.299436  -0.569953 -0.187123   \n",
      "\n",
      "   tryptophylglycine  xanthosine    uracil  ribulose/xylulose   xylose  \n",
      "0          -0.279174    0.554264  0.999394           0.955957  0.35008  \n",
      "-----------------------------------------line break----------\n",
      "   succinimide  tricarballylate  valylglutamine  valine betaine  \\\n",
      "0    -0.214061        -0.150998       -0.492243       -0.629533   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0                     -0.211035 -0.600983      -0.498594  -0.016599  0.651485   \n",
      "\n",
      "   tryptophylglycine  xanthosine  succinate    uracil  ribulose/xylulose  \\\n",
      "0          -0.341516   -0.712825  -0.569846 -0.116261           0.145819   \n",
      "\n",
      "     xylose  \n",
      "0 -0.642984  \n",
      "-----------------------------------------line break----------\n",
      "   tricarballylate  valylglutamine  valine betaine  \\\n",
      "0        -0.150998       -0.332883       -0.539738   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)   thymine  syringic acid  serotonin   ribitol  \\\n",
      "0                     -0.277557  2.711901      -0.492176  -0.345838 -1.050602   \n",
      "\n",
      "   tryptophylglycine  succinimide  xanthosine  succinate    uracil  \\\n",
      "0           0.057398    -1.234386    0.201495  -0.162229  2.527575   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0          -0.731801 -0.243722  \n",
      "-----------------------------------------line break----------\n",
      "   xanthosine    uracil  ribulose/xylulose  syringic acid  valylglutamine  \\\n",
      "0     -0.0906 -0.483208          -0.741389      -0.119642        -0.42639   \n",
      "\n",
      "   valine betaine  ursodeoxycholate sulfate (1)  tricarballylate   thymine  \\\n",
      "0        0.262149                      -0.47489         1.805186  0.402307   \n",
      "\n",
      "   serotonin   ribitol    xylose  tryptophylglycine  succinimide  succinate  \n",
      "0  -0.095854 -0.638526 -0.314156            0.91003     -1.80293   0.018159  \n",
      "-----------------------------------------line break----------\n",
      "   xanthosine    uracil  syringic acid  valylglutamine  valine betaine  \\\n",
      "0     -0.0906 -0.483208      -0.119642        0.029986        0.169974   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  serotonin  \\\n",
      "0                     -0.290837          0.18886  1.060455  -0.071973   \n",
      "\n",
      "    ribitol  tryptophylglycine  succinimide  succinate  ribulose/xylulose  \\\n",
      "0 -0.374138          -0.073697       0.2125  -0.867956           0.070327   \n",
      "\n",
      "     xylose  \n",
      "0 -0.068087  \n",
      "-----------------------------------------line break----------\n",
      "   xanthosine  syringic acid  valylglutamine  valine betaine  \\\n",
      "0     -0.0906      -0.119642        5.090126        0.410119   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  serotonin  \\\n",
      "0                     -0.503412        -0.694575 -0.721625  -0.347488   \n",
      "\n",
      "    ribitol  tryptophylglycine  succinimide    uracil  succinate  \\\n",
      "0 -0.284361           2.409446     1.922238 -0.868572   2.314943   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0           1.087766  0.706438  \n",
      "-----------------------------------------line break----------\n",
      "   syringic acid  valylglutamine  valine betaine  \\\n",
      "0      -0.119642        -0.33204       -0.869441   \n",
      "\n",
      "   ursodeoxycholate sulfate (1)  tricarballylate   thymine  serotonin  \\\n",
      "0                      1.678658        -0.537375  0.947437  -0.347488   \n",
      "\n",
      "    ribitol  tryptophylglycine  succinimide  xanthosine  succinate    uracil  \\\n",
      "0 -0.714117           0.889161     0.467085   -0.912142  -1.149255  0.358285   \n",
      "\n",
      "   ribulose/xylulose    xylose  \n",
      "0           0.299157  0.666042  \n",
      "-----------------------------------------line break----------\n"
     ]
    }
   ],
   "source": [
    "phi = ci.compute_modified_shap_proba(x_instance, is_classifier=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ci = FastCausalInference(data=X_train, model=model, target_variable='Prob_Class_1')\n",
    "#ci.load_causal_strengths(result_dir + 'Mean_Causal_Effect_IBS.json')\n",
    "\n",
    "phi = []\n",
    "for i in range(len(X_test)):\n",
    "    phi.append(ci.compute_modified_shap_proba(X_test.iloc[i], is_classifier=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_df = pd.DataFrame(phi)\n",
    "mean_values = phi_df.abs().mean()\n",
    "global_importance = mean_values.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"deletion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.8020833333333333,\n",
      "                0.7131410256410257,\n",
      "                0.6466346153846153,\n",
      "                0.6145833333333333,\n",
      "                0.6185897435897436,\n",
      "                0.5657051282051282,\n",
      "                0.5817307692307693,\n",
      "                0.594551282051282,\n",
      "                0.6386217948717949,\n",
      "                0.6073717948717949,\n",
      "                0.5945512820512819,\n",
      "                0.6378205128205129,\n",
      "                0.6422275641025641,\n",
      "                0.4911858974358975,\n",
      "                0.5\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.5333619228350892,\n",
      "                0.6176319966059503,\n",
      "                0.6528667436699289,\n",
      "                0.7458745657755351,\n",
      "                0.7544846458433045,\n",
      "                0.8056460519544488,\n",
      "                0.821400464724071,\n",
      "                0.847067025492969,\n",
      "                0.8662189324127925,\n",
      "                0.8891361917078303,\n",
      "                0.9339429947820997,\n",
      "                0.924163630982221,\n",
      "                0.9035509841238791,\n",
      "                0.8925618058423742,\n",
      "                0.9272027411818561\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.17575241929729749,\n",
      "                0.21478375157982196,\n",
      "                0.2307487920401132,\n",
      "                0.27604349501841946,\n",
      "                0.27991708339961313,\n",
      "                0.30461659114314243,\n",
      "                0.31193143333661116,\n",
      "                0.3233782880092635,\n",
      "                0.33164039930218103,\n",
      "                0.3413062780808007,\n",
      "                0.35965035941518275,\n",
      "                0.3559432155331778,\n",
      "                0.34736284251859567,\n",
      "                0.3432399848039314,\n",
      "                0.3581441230715909\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.6165865384615384,\n",
      "            \"cross_entropy\": 0.8076740465289568,\n",
      "            \"brier\": 0.3036306037699828,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    },\n",
      "    \"insertion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.8565705128205129,\n",
      "                0.8497596153846154,\n",
      "                0.9006410256410255,\n",
      "                0.8878205128205128,\n",
      "                0.8910256410256411,\n",
      "                0.8798076923076923,\n",
      "                0.8774038461538461,\n",
      "                0.8822115384615384,\n",
      "                0.8806089743589743,\n",
      "                0.8798076923076924,\n",
      "                0.8806089743589745,\n",
      "                0.8838141025641025,\n",
      "                0.8782051282051282,\n",
      "                0.8846153846153846,\n",
      "                0.8782051282051282\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.5577995700528187,\n",
      "                0.48561723474015095,\n",
      "                0.4648142548828526,\n",
      "                0.4380237734852441,\n",
      "                0.4442835781441994,\n",
      "                0.4428327871001319,\n",
      "                0.44765061538967194,\n",
      "                0.4426989729961006,\n",
      "                0.4442738758273558,\n",
      "                0.44550351090404394,\n",
      "                0.43598045275578146,\n",
      "                0.42746930445675824,\n",
      "                0.4283525816854086,\n",
      "                0.42277047467250956,\n",
      "                0.43130759728033224\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.18281475287193621,\n",
      "                0.15138956546440746,\n",
      "                0.14663547711080163,\n",
      "                0.13537964233817612,\n",
      "                0.13811109520936868,\n",
      "                0.13788975215072996,\n",
      "                0.13968135646795402,\n",
      "                0.13749926306716267,\n",
      "                0.13915692910978317,\n",
      "                0.13961429267355152,\n",
      "                0.1357247122086571,\n",
      "                0.13268623870064064,\n",
      "                0.1327547844285534,\n",
      "                0.13054367695291189,\n",
      "                0.13441740529050694\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.8794070512820512,\n",
      "            \"cross_entropy\": 0.4506252389582239,\n",
      "            \"brier\": 0.14095326293634275,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"deletion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.7875,\n",
      "                0.7458333333333332,\n",
      "                0.6716666666666666,\n",
      "                0.56,\n",
      "                0.5683333333333334,\n",
      "                0.5341666666666667,\n",
      "                0.4741666666666666,\n",
      "                0.4916666666666667,\n",
      "                0.5083333333333333,\n",
      "                0.4991666666666666,\n",
      "                0.46333333333333326,\n",
      "                0.47583333333333333,\n",
      "                0.48625,\n",
      "                0.6029166666666667,\n",
      "                0.5\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.5893880935369175,\n",
      "                0.6939687843745195,\n",
      "                0.739482983649083,\n",
      "                0.8458936405573276,\n",
      "                0.845303842443165,\n",
      "                0.8545702198313575,\n",
      "                0.8846286320184191,\n",
      "                0.8939189794493904,\n",
      "                0.9521357012719036,\n",
      "                0.9731932698454661,\n",
      "                1.0225135070200213,\n",
      "                1.0346153137679555,\n",
      "                1.0072793707727634,\n",
      "                0.8969212247236866,\n",
      "                0.9404186671352546\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.20014143968821002,\n",
      "                0.25060080888344277,\n",
      "                0.27206466693271686,\n",
      "                0.3204676327258146,\n",
      "                0.32018283899647154,\n",
      "                0.3243030181391419,\n",
      "                0.3375658266482527,\n",
      "                0.34093980394650514,\n",
      "                0.36567267078231797,\n",
      "                0.3739496219512934,\n",
      "                0.39332163692886773,\n",
      "                0.3976958840847826,\n",
      "                0.3878605485508814,\n",
      "                0.34589057140957685,\n",
      "                0.3649649438114703\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.5579444444444444,\n",
      "            \"cross_entropy\": 0.8782821486931487,\n",
      "            \"brier\": 0.3330414608986497,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    },\n",
      "    \"insertion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.7708333333333334,\n",
      "                0.7929166666666666,\n",
      "                0.8283333333333333,\n",
      "                0.8591666666666667,\n",
      "                0.86,\n",
      "                0.8566666666666666,\n",
      "                0.8641666666666666,\n",
      "                0.8633333333333333,\n",
      "                0.8475,\n",
      "                0.855,\n",
      "                0.8508333333333333,\n",
      "                0.8466666666666667,\n",
      "                0.8441666666666666,\n",
      "                0.8291666666666667,\n",
      "                0.8291666666666667\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.6613720387481131,\n",
      "                0.5813672929279904,\n",
      "                0.5390738351746055,\n",
      "                0.4728810069891966,\n",
      "                0.472941432601461,\n",
      "                0.4657083830235563,\n",
      "                0.46072975332157917,\n",
      "                0.4577855507947911,\n",
      "                0.46962335931889193,\n",
      "                0.47050102962014206,\n",
      "                0.47416295924753366,\n",
      "                0.47002089754970217,\n",
      "                0.47232662104160034,\n",
      "                0.4996235593909976,\n",
      "                0.4891898601283233\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.22809981569093526,\n",
      "                0.19605940381276157,\n",
      "                0.17778690431151167,\n",
      "                0.15332288574781724,\n",
      "                0.1531543107374633,\n",
      "                0.14966415303921526,\n",
      "                0.14896174104169768,\n",
      "                0.14710402713458515,\n",
      "                0.1504874565246574,\n",
      "                0.15078718043687392,\n",
      "                0.15248019862232925,\n",
      "                0.15122474485640175,\n",
      "                0.15293501061270628,\n",
      "                0.1631009832830725,\n",
      "                0.15935898123209713\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.8398611111111113,\n",
      "            \"cross_entropy\": 0.4971538386585656,\n",
      "            \"brier\": 0.16230185313894166,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"deletion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.8387096774193548,\n",
      "                0.7741935483870968,\n",
      "                0.7599399849962492,\n",
      "                0.7351837959489873,\n",
      "                0.741185296324081,\n",
      "                0.6954238559639909,\n",
      "                0.695423855963991,\n",
      "                0.6969242310577645,\n",
      "                0.6264066016504126,\n",
      "                0.6271567891972993,\n",
      "                0.568642160540135,\n",
      "                0.632033008252063,\n",
      "                0.6219054763690922,\n",
      "                0.5626406601650413,\n",
      "                0.5\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.5193068224943925,\n",
      "                0.5884276777641769,\n",
      "                0.606017463133041,\n",
      "                0.6769923018390795,\n",
      "                0.6740822002959469,\n",
      "                0.7092773128958064,\n",
      "                0.7261480090260554,\n",
      "                0.7375841733114205,\n",
      "                0.7708949145704211,\n",
      "                0.7821190731505933,\n",
      "                0.8683993068767332,\n",
      "                0.8760320874142727,\n",
      "                0.8815356593522089,\n",
      "                0.8507441265848273,\n",
      "                0.8937715432763311\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.16720830628118008,\n",
      "                0.20064877500497325,\n",
      "                0.20862298219047065,\n",
      "                0.24276897776608453,\n",
      "                0.24147858446148104,\n",
      "                0.2582368823258313,\n",
      "                0.26630388528749843,\n",
      "                0.27150835211641267,\n",
      "                0.2870313986237514,\n",
      "                0.2921601807702154,\n",
      "                0.3288839815908064,\n",
      "                0.3317336478636907,\n",
      "                0.33372936500829065,\n",
      "                0.3223428025595217,\n",
      "                0.3406323110321205\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.6717179294823705,\n",
      "            \"cross_entropy\": 0.7440888447990205,\n",
      "            \"brier\": 0.27288602885882185,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    },\n",
      "    \"insertion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.8945986496624156,\n",
      "                0.8807201800450112,\n",
      "                0.9024756189047263,\n",
      "                0.9167291822955739,\n",
      "                0.9114778694673669,\n",
      "                0.8979744936234059,\n",
      "                0.8829707426856714,\n",
      "                0.8784696174043511,\n",
      "                0.8807201800450112,\n",
      "                0.886721680420105,\n",
      "                0.8927231807951987,\n",
      "                0.8972243060765192,\n",
      "                0.8912228057014253,\n",
      "                0.8957239309827456,\n",
      "                0.8994748687171792\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.5339335987660522,\n",
      "                0.4644702556151713,\n",
      "                0.42012108956571426,\n",
      "                0.38370294811830236,\n",
      "                0.39664858026124095,\n",
      "                0.3982481630905456,\n",
      "                0.40193552863054616,\n",
      "                0.4066080191823932,\n",
      "                0.4040954930342047,\n",
      "                0.4035936477584196,\n",
      "                0.4019949745622336,\n",
      "                0.39591901362547177,\n",
      "                0.400364212711417,\n",
      "                0.3989797570020913,\n",
      "                0.40118807518577815\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.17395977258876588,\n",
      "                0.14233368561447618,\n",
      "                0.1271685787310535,\n",
      "                0.11529523911652789,\n",
      "                0.11956098236539048,\n",
      "                0.12093319311839477,\n",
      "                0.12201136849019512,\n",
      "                0.12159425470597084,\n",
      "                0.12040350540916127,\n",
      "                0.12090561827649288,\n",
      "                0.11986550681155662,\n",
      "                0.11772343355522369,\n",
      "                0.11923461935954412,\n",
      "                0.11867717145159401,\n",
      "                0.11986653926437998\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.8939484871217805,\n",
      "            \"cross_entropy\": 0.41412022380730545,\n",
      "            \"brier\": 0.1253022312572485,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"deletion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.8286445012787723,\n",
      "                0.7468030690537085,\n",
      "                0.7178175618073316,\n",
      "                0.6317135549872123,\n",
      "                0.6351236146632566,\n",
      "                0.5524296675191817,\n",
      "                0.4876385336743393,\n",
      "                0.5831202046035806,\n",
      "                0.5950554134697357,\n",
      "                0.5507246376811594,\n",
      "                0.463768115942029,\n",
      "                0.46675191815856787,\n",
      "                0.48508098891730606,\n",
      "                0.4671781756180733,\n",
      "                0.5\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.5247611733780392,\n",
      "                0.5992980891784528,\n",
      "                0.6364483622529632,\n",
      "                0.7030805887135134,\n",
      "                0.7123429769697089,\n",
      "                0.7792986083853992,\n",
      "                0.8037872866271865,\n",
      "                0.8157944287928084,\n",
      "                0.8312201302583456,\n",
      "                0.8499798908110113,\n",
      "                0.9004801982080393,\n",
      "                0.8711329850339488,\n",
      "                0.8632422438466738,\n",
      "                0.8482023882172938,\n",
      "                0.8697720218315294\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.17191309223617363,\n",
      "                0.20559231482901733,\n",
      "                0.22279499813175022,\n",
      "                0.2545604943132168,\n",
      "                0.25885472716700286,\n",
      "                0.2916288575515612,\n",
      "                0.30362085631261115,\n",
      "                0.3093286612325834,\n",
      "                0.3165553167224293,\n",
      "                0.3251276681479757,\n",
      "                0.3473243173536318,\n",
      "                0.33486828613858943,\n",
      "                0.3314188728675921,\n",
      "                0.3246201718081623,\n",
      "                0.3348469112139148\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.5807899971582837,\n",
      "            \"cross_entropy\": 0.7739227581669943,\n",
      "            \"brier\": 0.28887036973508085,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    },\n",
      "    \"insertion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.8047740835464622,\n",
      "                0.8405797101449275,\n",
      "                0.8350383631713556,\n",
      "                0.8644501278772379,\n",
      "                0.8678601875532822,\n",
      "                0.8635976129582268,\n",
      "                0.8687127024722933,\n",
      "                0.8584825234441603,\n",
      "                0.8431372549019608,\n",
      "                0.8465473145780051,\n",
      "                0.845694799658994,\n",
      "                0.8482523444160273,\n",
      "                0.8516624040920716,\n",
      "                0.8533674339300937,\n",
      "                0.8533674339300937\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.6304875596901828,\n",
      "                0.5231308843317846,\n",
      "                0.49239532464583374,\n",
      "                0.45061291867255465,\n",
      "                0.4464000778634561,\n",
      "                0.43741959341963843,\n",
      "                0.4343828088814684,\n",
      "                0.44095534195423597,\n",
      "                0.449821537239907,\n",
      "                0.4488176755567255,\n",
      "                0.44574551781082855,\n",
      "                0.4432763960927437,\n",
      "                0.4435487902067981,\n",
      "                0.44278392951614776,\n",
      "                0.45297774690534315\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.21120993032259455,\n",
      "                0.1662162563773505,\n",
      "                0.15263013176691798,\n",
      "                0.13865086965929263,\n",
      "                0.13745506200715107,\n",
      "                0.13346606119201282,\n",
      "                0.13267303433366406,\n",
      "                0.13544096246814463,\n",
      "                0.14009645039920346,\n",
      "                0.13952960336446998,\n",
      "                0.1379663286517446,\n",
      "                0.13606350263967296,\n",
      "                0.1369105188355207,\n",
      "                0.13671727850999613,\n",
      "                0.14064052329750873\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.849701619778346,\n",
      "            \"cross_entropy\": 0.4655170735191766,\n",
      "            \"brier\": 0.1450444342550163,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Training Random Forest model...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\snorl\\Desktop\\FYP\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"deletion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.77344336084021,\n",
      "                0.7531882970742686,\n",
      "                0.6946736684171043,\n",
      "                0.6721680420105026,\n",
      "                0.6984246061515379,\n",
      "                0.6631657914478619,\n",
      "                0.5836459114778695,\n",
      "                0.5656414103525882,\n",
      "                0.5468867216804201,\n",
      "                0.5142535633908477,\n",
      "                0.4343585896474118,\n",
      "                0.46061515378844714,\n",
      "                0.672543135783946,\n",
      "                0.5112528132033008,\n",
      "                0.5\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.5584202674130104,\n",
      "                0.6152823978620233,\n",
      "                0.6579896115602043,\n",
      "                0.7018821964848209,\n",
      "                0.7015595087141857,\n",
      "                0.7361052111439195,\n",
      "                0.809865226540508,\n",
      "                0.816834065043128,\n",
      "                0.8645194716902611,\n",
      "                0.8899206691652569,\n",
      "                0.9303423241854474,\n",
      "                0.9146826912239642,\n",
      "                0.8791741199636075,\n",
      "                0.857969859547468,\n",
      "                0.8770640441784052\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.18761853207735799,\n",
      "                0.21349258921102227,\n",
      "                0.23274374217265728,\n",
      "                0.2540702095284059,\n",
      "                0.2540927011858798,\n",
      "                0.27057281406661765,\n",
      "                0.30259065412606484,\n",
      "                0.30583363363363364,\n",
      "                0.32696897560060056,\n",
      "                0.3373056798048048,\n",
      "                0.35349237499999997,\n",
      "                0.3474291144894895,\n",
      "                0.33433060210210214,\n",
      "                0.32596126801801806,\n",
      "                0.3339753430930932\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.6029507376844212,\n",
      "            \"cross_entropy\": 0.7874407776477474,\n",
      "            \"brier\": 0.2920318822739832,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    },\n",
      "    \"insertion\": {\n",
      "        \"stepwise_metrics\": {\n",
      "            \"auroc\": [\n",
      "                0.7996999249812454,\n",
      "                0.8435858964741185,\n",
      "                0.8735933983495874,\n",
      "                0.8750937734433608,\n",
      "                0.8799699924981246,\n",
      "                0.8394598649662416,\n",
      "                0.8372093023255813,\n",
      "                0.8267066766691673,\n",
      "                0.8173293323330834,\n",
      "                0.8229557389347336,\n",
      "                0.8364591147786947,\n",
      "                0.837959489872468,\n",
      "                0.8315828957239311,\n",
      "                0.8289572393098275,\n",
      "                0.8244561140285072\n",
      "            ],\n",
      "            \"cross_entropy\": [\n",
      "                0.5568416944948791,\n",
      "                0.4803972268976447,\n",
      "                0.4527975866083388,\n",
      "                0.45266064480316187,\n",
      "                0.46507008443802283,\n",
      "                0.4697644738702588,\n",
      "                0.4766297945078694,\n",
      "                0.4832814192871908,\n",
      "                0.49814563592381705,\n",
      "                0.49581626002897194,\n",
      "                0.49149806755161485,\n",
      "                0.48823035493208217,\n",
      "                0.4942851453793706,\n",
      "                0.49850466599197213,\n",
      "                0.49914526465592496\n",
      "            ],\n",
      "            \"brier\": [\n",
      "                0.18404619594594598,\n",
      "                0.1502662864114114,\n",
      "                0.14139421585041675,\n",
      "                0.14019076074990808,\n",
      "                0.1450940666636024,\n",
      "                0.14834541533370105,\n",
      "                0.15032873661671875,\n",
      "                0.15207793787154503,\n",
      "                0.15736487452503525,\n",
      "                0.15694508265152907,\n",
      "                0.15690941367898512,\n",
      "                0.15577429668443954,\n",
      "                0.158317358138751,\n",
      "                0.1580167910250812,\n",
      "                0.15791030935718886\n",
      "            ],\n",
      "            \"rmse\": [],\n",
      "            \"mae\": [],\n",
      "            \"r_squared\": []\n",
      "        },\n",
      "        \"average_scores\": {\n",
      "            \"auroc\": 0.8383345836459115,\n",
      "            \"cross_entropy\": 0.4868712212914081,\n",
      "            \"brier\": 0.15419878276695062,\n",
      "            \"rmse\": NaN,\n",
      "            \"mae\": NaN,\n",
      "            \"r_squared\": NaN\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate_global_shap_scores\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 1010]\n",
    "\n",
    "X = X[[\"xylose\", \"xanthosine\", \"uracil\", \"ribulose/xylulose\", \"valylglutamine\", \"tryptophylglycine\", \"succinate\", \"valine betaine\", \"ursodeoxycholate sulfate (1)\", \"tricarballylate\",\"succinimide\", \"thymine\", \"syringic acid\", \"serotonin\", \"ribitol\" ]]\n",
    "\n",
    "y = df_encoded['Group']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X),columns=X.columns,index=X.index)\n",
    "\n",
    "all_scores = {\n",
    "    'deletion': {\n",
    "        'auroc': [],\n",
    "        'cross_entropy': [],\n",
    "        'brier': []\n",
    "    },\n",
    "    'insertion': {\n",
    "        'auroc': [],\n",
    "        'cross_entropy': [],\n",
    "        'brier': []\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in seeds:\n",
    "    print(\"Training Random Forest model...\")\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 7],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "    estimator=rf, param_distributions=param_dist, n_iter=50,\n",
    "            cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    shap_values = global_importance\n",
    "\n",
    "    result = evaluate_global_shap_scores(model, X_test, y_test, shap_values, causal=True)\n",
    "\n",
    "    for method in ['deletion', 'insertion']:\n",
    "        for metric in ['auroc', 'cross_entropy', 'brier']:\n",
    "            all_scores[method][metric].append(result[method][\"average_scores\"][metric])\n",
    "\n",
    "    import json\n",
    "\n",
    "    print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "{\n",
      "    \"deletion\": {\n",
      "        \"auroc\": {\n",
      "            \"mean\": 0.6059979294462116,\n",
      "            \"std\": 0.03843201710634803\n",
      "        },\n",
      "        \"cross_entropy\": {\n",
      "            \"mean\": 0.7982817151671735,\n",
      "            \"std\": 0.045024520350426006\n",
      "        },\n",
      "        \"brier\": {\n",
      "            \"mean\": 0.2980920691073037,\n",
      "            \"std\": 0.020046173807333888\n",
      "        }\n",
      "    },\n",
      "    \"insertion\": {\n",
      "        \"auroc\": {\n",
      "            \"mean\": 0.8602505705878402,\n",
      "            \"std\": 0.0224046478922976\n",
      "        },\n",
      "        \"cross_entropy\": {\n",
      "            \"mean\": 0.4628575192469359,\n",
      "            \"std\": 0.02927429345970162\n",
      "        },\n",
      "        \"brier\": {\n",
      "            \"mean\": 0.14556011287089998,\n",
      "            \"std\": 0.012544187841407898\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "final_results = {\n",
    "    'deletion': {\n",
    "        metric: {\n",
    "            'mean': np.mean(scores),\n",
    "            'std': np.std(scores)\n",
    "        }\n",
    "        for metric, scores in all_scores['deletion'].items()\n",
    "    },\n",
    "    'insertion': {\n",
    "        metric: {\n",
    "            'mean': np.mean(scores),\n",
    "            'std': np.std(scores)\n",
    "        }\n",
    "        for metric, scores in all_scores['insertion'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(json.dumps(final_results, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
